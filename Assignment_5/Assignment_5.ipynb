{"metadata":{"orig_nbformat":4,"colab":{"name":"Assignment_5.ipynb","provenance":[],"collapsed_sections":["UTyB-3McWOFa","6_guuzaKXGfr","9QaURe3mc4CB","n2bi4E5YfkzP","MMk82luBgPLW","ebqIYd1-gaPO","EcqgshmAgixU"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***NAME : VAIBHAV BILOTIA***\n# ***ROLL NO : 001811001036***\n# ***DEPARTMENT : INFORMATION TECHNOLOGY***\n# ***MACHINE LEARNING LAB***\n# ***ASSIGNMENT - 5***\n# ***GitHub Link: [Link](https://github.com/vaibhav1311/JU_IT_ML_Assignments/tree/main/Assignment_5)***","metadata":{"id":"AGvLitEJvvNE"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"uEk-RQVFxOKv"}},{"cell_type":"code","source":"!pip install --no-cache gym[all]\n!pip install IPython\n!pip install Box2D","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDbNX2_LvUC7","outputId":"ee2b0cfd-03de-4023-bec2-c617eee880f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.17.3)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\nRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.19.5)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.4.1)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.2.30)\nRequirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.2.9)\nRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.4.1)\nCollecting box2d-py~=2.3.5\n  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n\u001b[K     |████████████████████████████████| 448 kB 4.1 MB/s \n\u001b[?25hCollecting mujoco-py<2.0,>=1.50\n  Downloading mujoco-py-1.50.1.68.tar.gz (120 kB)\n\u001b[K     |████████████████████████████████| 120 kB 61.7 MB/s \n\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.1.2)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[all]) (1.15.0)\nCollecting glfw>=1.4.0\n  Downloading glfw-2.4.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (205 kB)\n\u001b[K     |████████████████████████████████| 205 kB 52.2 MB/s \n\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50->gym[all]) (0.29.24)\nRequirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50->gym[all]) (1.15.0)\nCollecting lockfile>=0.12.2\n  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50->gym[all]) (2.21)\nRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\nBuilding wheels for collected packages: mujoco-py\n  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n\u001b[?25h  Running setup.py clean for mujoco-py\nFailed to build mujoco-py\nInstalling collected packages: lockfile, glfw, mujoco-py, box2d-py\n    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-7uuogwls/mujoco-py_d8fc0a1325eb4c7e9700987ee559cab2/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-7uuogwls/mujoco-py_d8fc0a1325eb4c7e9700987ee559cab2/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-n5xniv2r/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/mujoco-py Check the logs for full command output.\u001b[0m\nRequirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (5.5.0)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython) (0.7.5)\nRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython) (4.4.2)\nRequirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython) (1.0.18)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython) (5.1.1)\nRequirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython) (0.8.1)\nRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython) (2.6.1)\nRequirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython) (4.8.0)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython) (57.4.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython) (1.15.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython) (0.2.5)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython) (0.7.0)\nCollecting Box2D\n  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n\u001b[K     |████████████████████████████████| 1.3 MB 3.7 MB/s \n\u001b[?25hInstalling collected packages: Box2D\nSuccessfully installed Box2D-2.3.10\n"}]},{"cell_type":"code","source":"from __future__ import print_function\nimport os, sys, time, datetime, json, random\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom tensorflow.keras.optimizers import SGD , Adam, RMSprop\nfrom keras.layers.advanced_activations import PReLU\nimport pylab as plt\nimport networkx as nx\nfrom keras import models\nfrom keras import layers\nfrom collections import deque\nimport random\n\nimport gym\nimport pickle\nfrom itertools import product\n\nfrom matplotlib.pyplot import cm\n\nfrom collections import defaultdict","metadata":{"id":"ZjP9ertAuAay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n#Shortest Path Using Reinforcement Learning\n","metadata":{"id":"UTyB-3McWOFa"}},{"cell_type":"code","source":"# map cell to cell, add circular cell to goal point\npoints_list = [(0,1), (2,5), (5,6), (4,5), (3,7), (2,3), (2,1)]","metadata":{"id":"a8sDrabXgvro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"goal = 7\n\nG=nx.Graph()\nG.add_edges_from(points_list)\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G,pos)\nnx.draw_networkx_edges(G,pos)\nnx.draw_networkx_labels(G,pos)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"nKUfdnkJg6hO","outputId":"3770c1ed-427a-425c-e7d1-3914fb744b22"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"# how many points in graph? x points\nMATRIX_SIZE = 8\n\n# create matrix x*y\nR = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))\nR *= -1","metadata":{"id":"nD7ggaAHhhTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assign zeros to paths and 100 to goal-reaching point\nfor point in points_list:\n    print(point)\n    if point[1] == goal:\n        R[point] = 100\n    else:\n        R[point] = 0\n\n    if point[0] == goal:\n        R[point[::-1]] = 100\n    else:\n        R[point[::-1]]= 0\n\nR[goal,goal]= 100\n\nprint(R)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dr14Sb3vr6aN","outputId":"67c8c98d-3671-4cee-c18d-93a094186ade"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"(0, 1)\n(2, 5)\n(5, 6)\n(4, 5)\n(3, 7)\n(2, 3)\n(2, 1)\n[[ -1.   0.  -1.  -1.  -1.  -1.  -1.  -1.]\n [  0.  -1.   0.  -1.  -1.  -1.  -1.  -1.]\n [ -1.   0.  -1.   0.  -1.   0.  -1.  -1.]\n [ -1.  -1.   0.  -1.  -1.  -1.  -1. 100.]\n [ -1.  -1.  -1.  -1.  -1.   0.  -1.  -1.]\n [ -1.  -1.   0.  -1.   0.  -1.   0.  -1.]\n [ -1.  -1.  -1.  -1.  -1.   0.  -1.  -1.]\n [ -1.  -1.  -1.   0.  -1.  -1.  -1. 100.]]\n"}]},{"cell_type":"code","source":"Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n\n# learning parameter\ngamma = 0.8\n\ninitial_state = 1\n\ndef available_actions(state):\n    current_state_row = R[state,]\n    av_act = np.where(current_state_row >= 0)[1]\n    return av_act\n\navailable_act = available_actions(initial_state)\n\ndef sample_next_action(available_actions_range):\n    next_action = int(np.random.choice(available_act,1))\n    return next_action\n\naction = sample_next_action(available_act)\n\ndef update(current_state, action, gamma):\n\n  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n\n  if max_index.shape[0] > 1:\n      max_index = int(np.random.choice(max_index, size = 1))\n  else:\n      max_index = int(max_index)\n  max_value = Q[action, max_index]\n\n  Q[current_state, action] = R[current_state, action] + gamma * max_value\n  print('max_value', R[current_state, action] + gamma * max_value)\n\n  if (np.max(Q) > 0):\n    return(np.sum(Q/np.max(Q)*100))\n  else:\n    return (0)\n\nupdate(initial_state, action, gamma)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_SzGu3HsQK3","outputId":"8b80a3ae-610c-46a5-bb78-3be157d79b9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"max_value 0.0\n"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Training\nscores = []\nfor i in range(700):\n    current_state = np.random.randint(0, int(Q.shape[0]))\n    available_act = available_actions(current_state)\n    action = sample_next_action(available_act)\n    score = update(current_state,action,gamma)\n    scores.append(score)\n    print ('Score:', str(score))\n\nprint(\"Trained Q matrix:\")\nprint(Q/np.max(Q)*100)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1N5rX3zUwO4O","outputId":"e575689f-fb4c-4522-c367-be106ec075c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"max_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 0.0\nScore: 0\nmax_value 100.0\nScore: 100.0\nmax_value 0.0\nScore: 100.0\nmax_value 0.0\nScore: 100.0\nmax_value 80.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 0.0\nScore: 180.0\nmax_value 164.0\nScore: 209.7560975609756\nmax_value 0.0\nScore: 209.7560975609756\nmax_value 231.20000000000002\nScore: 177.85467128027682\nmax_value 0.0\nScore: 177.85467128027682\nmax_value 0.0\nScore: 177.85467128027682\nmax_value 0.0\nScore: 177.85467128027682\nmax_value 80.0\nScore: 177.85467128027682\nmax_value 0.0\nScore: 177.85467128027682\nmax_value 284.96000000000004\nScore: 209.20830993823694\nmax_value 0.0\nScore: 209.20830993823694\nmax_value 284.96000000000004\nScore: 209.20830993823694\nmax_value 0.0\nScore: 209.20830993823694\nmax_value 0.0\nScore: 209.20830993823694\nmax_value 0.0\nScore: 209.20830993823694\nmax_value 0.0\nScore: 209.20830993823694\nmax_value 0.0\nScore: 209.20830993823694\nmax_value 227.96800000000005\nScore: 261.1341942728804\nmax_value 284.96000000000004\nScore: 261.1341942728804\nmax_value 0.0\nScore: 261.1341942728804\nmax_value 0.0\nScore: 261.1341942728804\nmax_value 0.0\nScore: 261.1341942728804\nmax_value 284.96000000000004\nScore: 280.0\nmax_value 327.9680000000001\nScore: 256.39574592643186\nmax_value 0.0\nScore: 256.39574592643186\nmax_value 0.0\nScore: 256.39574592643186\nmax_value 262.3744000000001\nScore: 336.39574592643186\nmax_value 0.0\nScore: 336.39574592643186\nmax_value 0.0\nScore: 336.39574592643186\nmax_value 209.89952000000008\nScore: 400.39574592643186\nmax_value 0.0\nScore: 400.39574592643186\nmax_value 0.0\nScore: 400.39574592643186\nmax_value 209.89952000000008\nScore: 464.39574592643186\nmax_value 0.0\nScore: 464.39574592643186\nmax_value 167.91961600000008\nScore: 515.5957459264318\nmax_value 134.33569280000006\nScore: 556.5557459264319\nmax_value 0.0\nScore: 556.5557459264319\nmax_value 327.9680000000001\nScore: 569.6692204117476\nmax_value 0.0\nScore: 569.6692204117476\nmax_value 167.91961600000008\nScore: 620.8692204117475\nmax_value 167.91961600000008\nScore: 672.0692204117474\nmax_value 209.89952000000008\nScore: 672.0692204117474\nmax_value 209.89952000000008\nScore: 672.0692204117474\nmax_value 362.3744000000001\nScore: 617.7529043994277\nmax_value 209.89952000000008\nScore: 675.6762897158299\nmax_value 167.91961600000008\nScore: 675.6762897158299\nmax_value 209.89952000000008\nScore: 675.6762897158299\nmax_value 362.3744000000001\nScore: 675.6762897158299\nmax_value 134.33569280000006\nScore: 712.7472563183271\nmax_value 209.89952000000008\nScore: 712.7472563183271\nmax_value 209.89952000000008\nScore: 712.7472563183271\nmax_value 167.91961600000008\nScore: 712.7472563183271\nmax_value 167.91961600000008\nScore: 712.7472563183271\nmax_value 167.91961600000008\nScore: 712.7472563183271\nmax_value 362.3744000000001\nScore: 722.241966761449\nmax_value 134.33569280000006\nScore: 759.3129333639463\nmax_value 209.89952000000008\nScore: 759.3129333639463\nmax_value 167.91961600000008\nScore: 759.3129333639463\nmax_value 209.89952000000008\nScore: 759.3129333639463\nmax_value 134.33569280000006\nScore: 759.3129333639463\nmax_value 167.91961600000008\nScore: 805.651641617068\nmax_value 167.91961600000008\nScore: 805.651641617068\nmax_value 209.89952000000008\nScore: 805.651641617068\nmax_value 167.91961600000008\nScore: 805.651641617068\nmax_value 167.91961600000008\nScore: 805.651641617068\nmax_value 209.89952000000008\nScore: 805.651641617068\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 209.89952000000008\nScore: 851.9903498701897\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 209.89952000000008\nScore: 851.9903498701897\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 209.89952000000008\nScore: 851.9903498701897\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 167.91961600000008\nScore: 851.9903498701897\nmax_value 289.8995200000001\nScore: 859.5861182246871\nmax_value 167.91961600000008\nScore: 859.5861182246871\nmax_value 389.8995200000001\nScore: 805.9628179075471\nmax_value 167.91961600000008\nScore: 805.9628179075471\nmax_value 167.91961600000008\nScore: 805.9628179075471\nmax_value 167.91961600000008\nScore: 805.9628179075471\nmax_value 389.8995200000001\nScore: 805.9628179075471\nmax_value 134.33569280000006\nScore: 805.9628179075471\nmax_value 167.91961600000008\nScore: 805.9628179075471\nmax_value 167.91961600000008\nScore: 805.9628179075471\nmax_value 231.9196160000001\nScore: 811.6104514311791\nmax_value 167.91961600000008\nScore: 811.6104514311791\nmax_value 389.8995200000001\nScore: 811.6104514311791\nmax_value 167.91961600000008\nScore: 811.6104514311791\nmax_value 167.91961600000008\nScore: 811.6104514311791\nmax_value 167.91961600000008\nScore: 811.6104514311791\nmax_value 134.33569280000006\nScore: 811.6104514311791\nmax_value 134.33569280000006\nScore: 811.6104514311791\nmax_value 311.91961600000013\nScore: 817.258084954811\nmax_value 167.91961600000008\nScore: 817.258084954811\nmax_value 311.91961600000013\nScore: 817.258084954811\nmax_value 249.5356928000001\nScore: 827.4238252973485\nmax_value 199.62855424000008\nScore: 835.5564175713785\nmax_value 249.5356928000001\nScore: 845.722157913916\nmax_value 199.62855424000008\nScore: 845.722157913916\nmax_value 199.62855424000008\nScore: 853.854750187946\nmax_value 159.7028433920001\nScore: 860.36082400717\nmax_value 159.7028433920001\nScore: 860.36082400717\nmax_value 249.5356928000001\nScore: 860.36082400717\nmax_value 199.62855424000008\nScore: 868.4934162812\nmax_value 389.8995200000001\nScore: 875.5529581857398\nmax_value 199.62855424000008\nScore: 875.5529581857398\nmax_value 199.62855424000008\nScore: 875.5529581857398\nmax_value 249.5356928000001\nScore: 875.5529581857398\nmax_value 199.62855424000008\nScore: 875.5529581857398\nmax_value 199.62855424000008\nScore: 875.5529581857398\nmax_value 199.62855424000008\nScore: 875.5529581857398\nmax_value 199.62855424000008\nScore: 875.5529581857398\nmax_value 411.91961600000013\nScore: 834.0940183125439\nmax_value 199.62855424000008\nScore: 841.79186444765\nmax_value 329.5356928000001\nScore: 846.06844563382\nmax_value 411.91961600000013\nScore: 846.06844563382\nmax_value 159.7028433920001\nScore: 846.06844563382\nmax_value 199.62855424000008\nScore: 846.06844563382\nmax_value 199.62855424000008\nScore: 853.7662917689262\nmax_value 263.6285542400001\nScore: 857.1875567178622\nmax_value 210.90284339200008\nScore: 859.924568677011\nmax_value 159.7028433920001\nScore: 866.0828455850959\nmax_value 263.6285542400001\nScore: 866.0828455850959\nmax_value 159.7028433920001\nScore: 872.2411224931808\nmax_value 263.6285542400001\nScore: 872.2411224931808\nmax_value 199.62855424000008\nScore: 872.2411224931808\nmax_value 159.7028433920001\nScore: 872.2411224931808\nmax_value 159.7028433920001\nScore: 872.2411224931808\nmax_value 199.62855424000008\nScore: 872.2411224931808\nmax_value 199.62855424000008\nScore: 872.2411224931808\nmax_value 199.62855424000008\nScore: 872.2411224931808\nmax_value 199.62855424000008\nScore: 872.2411224931808\nmax_value 159.7028433920001\nScore: 872.2411224931808\nmax_value 199.62855424000008\nScore: 872.2411224931808\nmax_value 263.6285542400001\nScore: 872.2411224931808\nmax_value 168.7222747136001\nScore: 874.4307320604998\nmax_value 263.6285542400001\nScore: 882.128578195606\nmax_value 411.91961600000013\nScore: 887.4743046783186\nmax_value 263.6285542400001\nScore: 890.8955696272546\nmax_value 429.5356928000001\nScore: 858.4594360791617\nmax_value 429.5356928000001\nScore: 858.4594360791617\nmax_value 210.90284339200008\nScore: 861.084197965306\nmax_value 263.6285542400001\nScore: 861.084197965306\nmax_value 210.90284339200008\nScore: 863.7089598514501\nmax_value 159.7028433920001\nScore: 863.7089598514501\nmax_value 168.7222747136001\nScore: 865.8087693603657\nmax_value 429.5356928000001\nScore: 869.909959807466\nmax_value 159.7028433920001\nScore: 869.909959807466\nmax_value 263.6285542400001\nScore: 869.909959807466\nmax_value 168.7222747136001\nScore: 869.909959807466\nmax_value 168.7222747136001\nScore: 869.909959807466\nmax_value 343.6285542400001\nScore: 896.8368383367093\nmax_value 159.7028433920001\nScore: 896.8368383367093\nmax_value 210.90284339200008\nScore: 899.4616002228536\nmax_value 263.6285542400001\nScore: 899.4616002228536\nmax_value 210.90284339200008\nScore: 899.4616002228536\nmax_value 168.7222747136001\nScore: 899.4616002228536\nmax_value 210.90284339200008\nScore: 899.4616002228536\nmax_value 168.7222747136001\nScore: 899.4616002228536\nmax_value 210.90284339200008\nScore: 899.4616002228536\nmax_value 210.90284339200008\nScore: 902.0863621089977\nmax_value 168.7222747136001\nScore: 904.1861716179133\nmax_value 168.7222747136001\nScore: 904.1861716179133\nmax_value 168.7222747136001\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 263.6285542400001\nScore: 904.1861716179133\nmax_value 263.6285542400001\nScore: 904.1861716179133\nmax_value 263.6285542400001\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 210.90284339200008\nScore: 904.1861716179133\nmax_value 443.6285542400001\nScore: 878.6393843783252\nmax_value 263.6285542400001\nScore: 878.6393843783252\nmax_value 210.90284339200008\nScore: 878.6393843783252\nmax_value 210.90284339200008\nScore: 878.6393843783252\nmax_value 210.90284339200008\nScore: 878.6393843783252\nmax_value 263.6285542400001\nScore: 878.6393843783252\nmax_value 210.90284339200008\nScore: 878.6393843783252\nmax_value 168.7222747136001\nScore: 878.6393843783252\nmax_value 210.90284339200008\nScore: 878.6393843783252\nmax_value 168.7222747136001\nScore: 878.6393843783252\nmax_value 343.6285542400001\nScore: 878.6393843783252\nmax_value 263.6285542400001\nScore: 878.6393843783252\nmax_value 210.90284339200008\nScore: 878.6393843783252\nmax_value 343.6285542400001\nScore: 881.8161098855783\nmax_value 168.7222747136001\nScore: 881.8161098855783\nmax_value 343.6285542400001\nScore: 881.8161098855783\nmax_value 210.90284339200008\nScore: 881.8161098855783\nmax_value 210.90284339200008\nScore: 881.8161098855783\nmax_value 210.90284339200008\nScore: 881.8161098855783\nmax_value 274.9028433920001\nScore: 884.3574902913807\nmax_value 454.9028433920001\nScore: 868.0160074713311\nmax_value 210.90284339200008\nScore: 868.0160074713311\nmax_value 210.90284339200008\nScore: 868.0160074713311\nmax_value 210.90284339200008\nScore: 868.0160074713311\nmax_value 210.90284339200008\nScore: 868.0160074713311\nmax_value 210.90284339200008\nScore: 868.0160074713311\nmax_value 219.92227471360007\nScore: 869.9987234408218\nmax_value 210.90284339200008\nScore: 869.9987234408218\nmax_value 219.92227471360007\nScore: 871.9814394103123\nmax_value 210.90284339200008\nScore: 871.9814394103123\nmax_value 210.90284339200008\nScore: 871.9814394103123\nmax_value 210.90284339200008\nScore: 871.9814394103123\nmax_value 168.7222747136001\nScore: 871.9814394103123\nmax_value 219.92227471360007\nScore: 873.964155379803\nmax_value 363.9222747136001\nScore: 878.4252663111567\nmax_value 363.9222747136001\nScore: 882.8863772425107\nmax_value 291.13781977088007\nScore: 888.9336609494569\nmax_value 210.90284339200008\nScore: 888.9336609494569\nmax_value 291.13781977088007\nScore: 892.50254969454\nmax_value 363.9222747136001\nScore: 892.50254969454\nmax_value 232.91025581670408\nScore: 895.3576606906065\nmax_value 232.91025581670408\nScore: 898.2127716866729\nmax_value 232.91025581670408\nScore: 898.2127716866729\nmax_value 454.9028433920001\nScore: 898.2127716866729\nmax_value 175.93781977088008\nScore: 899.7989444622655\nmax_value 210.90284339200008\nScore: 899.7989444622655\nmax_value 210.90284339200008\nScore: 899.7989444622655\nmax_value 186.3282046533633\nScore: 903.6692060347111\nmax_value 232.91025581670408\nScore: 903.6692060347111\nmax_value 168.7222747136001\nScore: 903.6692060347111\nmax_value 363.9222747136001\nScore: 903.6692060347111\nmax_value 291.13781977088007\nScore: 909.7164897416575\nmax_value 168.7222747136001\nScore: 909.7164897416575\nmax_value 291.13781977088007\nScore: 909.7164897416575\nmax_value 232.91025581670408\nScore: 914.5543167072145\nmax_value 291.13781977088007\nScore: 914.5543167072145\nmax_value 232.91025581670408\nScore: 917.4094277032809\nmax_value 291.13781977088007\nScore: 917.4094277032809\nmax_value 232.91025581670408\nScore: 917.4094277032809\nmax_value 232.91025581670408\nScore: 917.4094277032809\nmax_value 232.91025581670408\nScore: 917.4094277032809\nmax_value 454.9028433920001\nScore: 917.4094277032809\nmax_value 454.9028433920001\nScore: 917.4094277032809\nmax_value 232.91025581670408\nScore: 917.4094277032809\nmax_value 291.13781977088007\nScore: 917.4094277032809\nmax_value 186.3282046533633\nScore: 919.6935165001341\nmax_value 232.91025581670408\nScore: 919.6935165001341\nmax_value 291.13781977088007\nScore: 919.6935165001341\nmax_value 186.3282046533633\nScore: 919.6935165001341\nmax_value 186.3282046533633\nScore: 919.6935165001341\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 291.13781977088007\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 186.3282046533633\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 291.13781977088007\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 291.13781977088007\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 291.13781977088007\nScore: 924.5313434656912\nmax_value 232.91025581670408\nScore: 924.5313434656912\nmax_value 454.9028433920001\nScore: 927.0097384275543\nmax_value 363.9222747136001\nScore: 927.0097384275543\nmax_value 291.13781977088007\nScore: 927.0097384275543\nmax_value 186.3282046533633\nScore: 927.0097384275543\nmax_value 291.13781977088007\nScore: 927.0097384275543\nmax_value 291.13781977088007\nScore: 927.0097384275543\nmax_value 232.91025581670408\nScore: 927.0097384275543\nmax_value 186.3282046533633\nScore: 930.8800000000001\nmax_value 291.13781977088007\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 291.13781977088007\nScore: 930.8800000000001\nmax_value 186.3282046533633\nScore: 930.8800000000001\nmax_value 291.13781977088007\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 291.13781977088007\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 291.13781977088007\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 232.91025581670408\nScore: 930.8800000000001\nmax_value 291.13781977088007\nScore: 930.8800000000001\nmax_value 186.3282046533633\nScore: 930.8800000000001\nmax_value 463.9222747136001\nScore: 914.7262917066929\nmax_value 463.9222747136001\nScore: 914.7262917066929\nmax_value 232.91025581670408\nScore: 914.7262917066929\nmax_value 232.91025581670408\nScore: 914.7262917066929\nmax_value 371.13781977088007\nScore: 916.2816266088023\nmax_value 463.9222747136001\nScore: 916.2816266088023\nmax_value 232.91025581670408\nScore: 916.2816266088023\nmax_value 232.91025581670408\nScore: 916.2816266088023\nmax_value 296.9102558167041\nScore: 917.5258945304897\nmax_value 237.52820465336328\nScore: 918.5213088678397\nmax_value 371.13781977088007\nScore: 920.076643769949\nmax_value 371.13781977088007\nScore: 920.076643769949\nmax_value 232.91025581670408\nScore: 920.076643769949\nmax_value 237.52820465336328\nScore: 921.0720581072989\nmax_value 296.9102558167041\nScore: 922.3163260289863\nmax_value 237.52820465336328\nScore: 922.3163260289863\nmax_value 296.9102558167041\nScore: 923.5605939506737\nmax_value 463.9222747136001\nScore: 925.5047625783104\nmax_value 296.9102558167041\nScore: 925.5047625783104\nmax_value 237.52820465336328\nScore: 926.5001769156604\nmax_value 237.52820465336328\nScore: 926.5001769156604\nmax_value 237.52820465336328\nScore: 926.5001769156604\nmax_value 237.52820465336328\nScore: 926.5001769156604\nmax_value 237.52820465336328\nScore: 926.5001769156604\nmax_value 237.52820465336328\nScore: 926.5001769156604\nmax_value 190.02256372269062\nScore: 927.2965083855403\nmax_value 237.52820465336328\nScore: 927.2965083855403\nmax_value 237.52820465336328\nScore: 927.2965083855403\nmax_value 237.52820465336328\nScore: 928.2919227228901\nmax_value 296.9102558167041\nScore: 928.2919227228901\nmax_value 237.52820465336328\nScore: 928.2919227228901\nmax_value 237.52820465336328\nScore: 928.2919227228901\nmax_value 237.52820465336328\nScore: 928.2919227228901\nmax_value 190.02256372269062\nScore: 928.2919227228901\nmax_value 237.52820465336328\nScore: 928.2919227228901\nmax_value 237.52820465336328\nScore: 928.2919227228901\nmax_value 190.02256372269062\nScore: 928.2919227228901\nmax_value 296.9102558167041\nScore: 928.2919227228901\nmax_value 471.13781977088007\nScore: 915.6065100088471\nmax_value 237.52820465336328\nScore: 915.6065100088471\nmax_value 190.02256372269062\nScore: 916.3906455450526\nmax_value 296.9102558167041\nScore: 916.3906455450526\nmax_value 371.13781977088007\nScore: 916.3906455450526\nmax_value 237.52820465336328\nScore: 916.3906455450526\nmax_value 296.9102558167041\nScore: 916.3906455450526\nmax_value 476.9102558167041\nScore: 906.509199859328\nmax_value 190.02256372269062\nScore: 906.509199859328\nmax_value 371.13781977088007\nScore: 906.509199859328\nmax_value 371.13781977088007\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 371.13781977088007\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 296.9102558167041\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 296.9102558167041\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 371.13781977088007\nScore: 906.509199859328\nmax_value 237.52820465336328\nScore: 906.509199859328\nmax_value 190.02256372269062\nScore: 907.2838443598956\nmax_value 190.02256372269062\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 371.13781977088007\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 190.02256372269062\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 190.02256372269062\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 371.13781977088007\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 237.52820465336328\nScore: 907.2838443598956\nmax_value 296.9102558167041\nScore: 907.2838443598956\nmax_value 190.02256372269062\nScore: 907.2838443598956\nmax_value 481.5282046533633\nScore: 899.5418357840964\nmax_value 371.13781977088007\nScore: 899.5418357840964\nmax_value 296.9102558167041\nScore: 899.5418357840964\nmax_value 237.52820465336328\nScore: 899.5418357840964\nmax_value 296.9102558167041\nScore: 899.5418357840964\nmax_value 190.02256372269062\nScore: 899.5418357840964\nmax_value 237.52820465336328\nScore: 899.5418357840964\nmax_value 296.9102558167041\nScore: 899.5418357840964\nmax_value 237.52820465336328\nScore: 899.5418357840964\nmax_value 296.9102558167041\nScore: 899.5418357840964\nmax_value 371.13781977088007\nScore: 899.5418357840964\nmax_value 485.22256372269067\nScore: 893.4543310941376\nmax_value 237.52820465336328\nScore: 893.4543310941376\nmax_value 237.52820465336328\nScore: 893.4543310941376\nmax_value 296.9102558167041\nScore: 893.4543310941376\nmax_value 488.17805097815256\nScore: 893.0138867480638\nmax_value 237.52820465336328\nScore: 893.0138867480638\nmax_value 296.9102558167041\nScore: 893.0138867480638\nmax_value 237.52820465336328\nScore: 893.0138867480638\nmax_value 237.52820465336328\nScore: 893.0138867480638\nmax_value 390.5424407825221\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 390.5424407825221\nScore: 896.9887932340807\nmax_value 296.9102558167041\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 190.02256372269062\nScore: 896.9887932340807\nmax_value 190.02256372269062\nScore: 896.9887932340807\nmax_value 390.5424407825221\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 296.9102558167041\nScore: 896.9887932340807\nmax_value 237.52820465336328\nScore: 896.9887932340807\nmax_value 390.5424407825221\nScore: 900.9636997200978\nmax_value 237.52820465336328\nScore: 900.9636997200978\nmax_value 237.52820465336328\nScore: 900.9636997200978\nmax_value 312.4339526260177\nScore: 904.1436249089114\nmax_value 488.17805097815256\nScore: 904.749036653569\nmax_value 390.5424407825221\nScore: 904.749036653569\nmax_value 490.5424407825221\nScore: 900.8701869167254\nmax_value 312.4339526260177\nScore: 904.0347850266362\nmax_value 312.4339526260177\nScore: 907.1993831365469\nmax_value 249.94716210081415\nScore: 909.7310616244754\nmax_value 312.4339526260177\nScore: 909.7310616244754\nmax_value 390.5424407825221\nScore: 909.7310616244754\nmax_value 390.5424407825221\nScore: 909.7310616244754\nmax_value 249.94716210081415\nScore: 912.2627401124039\nmax_value 249.94716210081415\nScore: 912.2627401124039\nmax_value 249.94716210081415\nScore: 912.2627401124039\nmax_value 492.4339526260177\nScore: 909.6228620016777\nmax_value 312.4339526260177\nScore: 909.6228620016777\nmax_value 249.94716210081415\nScore: 912.1448159370682\nmax_value 199.95772968065134\nScore: 914.1623790853806\nmax_value 249.94716210081415\nScore: 914.1623790853806\nmax_value 249.94716210081415\nScore: 914.1623790853806\nmax_value 249.94716210081415\nScore: 914.1623790853806\nmax_value 249.94716210081415\nScore: 914.1623790853806\nmax_value 312.4339526260177\nScore: 914.1623790853806\nmax_value 492.4339526260177\nScore: 914.5464939160922\nmax_value 249.94716210081415\nScore: 914.5464939160922\nmax_value 249.94716210081415\nScore: 917.0684478514827\nmax_value 199.95772968065134\nScore: 919.0860109997951\nmax_value 199.95772968065134\nScore: 919.0860109997951\nmax_value 249.94716210081415\nScore: 919.0860109997951\nmax_value 393.9471621008142\nScore: 919.7774176950759\nmax_value 393.9471621008142\nScore: 920.4688243903566\nmax_value 315.15772968065136\nScore: 921.0219497465813\nmax_value 249.94716210081415\nScore: 921.0219497465813\nmax_value 199.95772968065134\nScore: 921.0219497465813\nmax_value 249.94716210081415\nScore: 921.0219497465813\nmax_value 199.95772968065134\nScore: 921.0219497465813\nmax_value 199.95772968065134\nScore: 921.0219497465813\nmax_value 199.95772968065134\nScore: 921.0219497465813\nmax_value 315.15772968065136\nScore: 921.0219497465813\nmax_value 199.95772968065134\nScore: 923.0395128948937\nmax_value 249.94716210081415\nScore: 923.0395128948937\nmax_value 199.95772968065134\nScore: 923.0395128948937\nmax_value 252.1261837445211\nScore: 923.4820131798733\nmax_value 493.9471621008142\nScore: 920.9592721252059\nmax_value 495.15772968065136\nScore: 919.2577825165002\nmax_value 199.95772968065134\nScore: 919.2577825165002\nmax_value 199.95772968065134\nScore: 919.2577825165002\nmax_value 249.94716210081415\nScore: 919.2577825165002\nmax_value 249.94716210081415\nScore: 919.2577825165002\nmax_value 249.94716210081415\nScore: 919.2577825165002\nmax_value 249.94716210081415\nScore: 919.2577825165002\nmax_value 249.94716210081415\nScore: 919.2577825165002\nmax_value 315.15772968065136\nScore: 919.8078652257797\nmax_value 252.1261837445211\nScore: 920.2479313932032\nmax_value 252.1261837445211\nScore: 920.6879975606267\nmax_value 252.1261837445211\nScore: 920.6879975606267\nmax_value 252.1261837445211\nScore: 921.1280637280502\nmax_value 252.1261837445211\nScore: 921.1280637280502\nmax_value 252.1261837445211\nScore: 921.1280637280502\nmax_value 252.1261837445211\nScore: 921.1280637280502\nmax_value 201.70094699561687\nScore: 921.4801166619891\nmax_value 252.1261837445211\nScore: 921.4801166619891\nmax_value 252.1261837445211\nScore: 921.4801166619891\nmax_value 252.1261837445211\nScore: 921.4801166619891\nmax_value 201.70094699561687\nScore: 921.8321695959278\nmax_value 252.1261837445211\nScore: 921.8321695959278\nmax_value 252.1261837445211\nScore: 921.8321695959278\nmax_value 252.1261837445211\nScore: 921.8321695959278\nmax_value 315.15772968065136\nScore: 921.8321695959278\nmax_value 252.1261837445211\nScore: 921.8321695959278\nmax_value 396.1261837445211\nScore: 922.2722357633513\nmax_value 201.70094699561687\nScore: 922.2722357633513\nmax_value 252.1261837445211\nScore: 922.2722357633513\nmax_value 252.1261837445211\nScore: 922.2722357633513\nmax_value 396.1261837445211\nScore: 922.2722357633513\nmax_value 315.15772968065136\nScore: 922.2722357633513\nmax_value 315.15772968065136\nScore: 922.2722357633513\nmax_value 495.15772968065136\nScore: 922.2722357633513\nmax_value 252.1261837445211\nScore: 922.2722357633513\nmax_value 201.70094699561687\nScore: 922.2722357633513\nmax_value 252.1261837445211\nScore: 922.2722357633513\nmax_value 396.1261837445211\nScore: 922.2722357633513\nmax_value 252.1261837445211\nScore: 922.2722357633513\nmax_value 252.1261837445211\nScore: 922.2722357633513\nmax_value 315.15772968065136\nScore: 922.2722357633513\nmax_value 201.70094699561687\nScore: 922.62428869729\nmax_value 315.15772968065136\nScore: 922.62428869729\nmax_value 252.1261837445211\nScore: 922.62428869729\nmax_value 315.15772968065136\nScore: 922.62428869729\nmax_value 201.70094699561687\nScore: 922.62428869729\nmax_value 315.15772968065136\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 315.15772968065136\nScore: 923.1743714065694\nmax_value 315.15772968065136\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 495.15772968065136\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 201.70094699561687\nScore: 923.1743714065694\nmax_value 201.70094699561687\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 252.1261837445211\nScore: 923.1743714065694\nmax_value 315.15772968065136\nScore: 923.1743714065694\nmax_value 495.15772968065136\nScore: 923.4188526106936\nmax_value 252.1261837445211\nScore: 923.4188526106936\nmax_value 315.15772968065136\nScore: 923.4188526106936\nmax_value 315.15772968065136\nScore: 923.4188526106936\nmax_value 496.1261837445211\nScore: 921.8115128648672\nmax_value 201.70094699561687\nScore: 921.8115128648672\nmax_value 396.1261837445211\nScore: 921.8115128648672\nmax_value 201.70094699561687\nScore: 921.8115128648672\nmax_value 396.1261837445211\nScore: 921.8115128648672\nmax_value 496.9009469956169\nScore: 920.5301521362454\nmax_value 315.15772968065136\nScore: 920.5301521362454\nmax_value 497.5207575964935\nScore: 919.507936924382\nmax_value 396.1261837445211\nScore: 919.507936924382\nmax_value 315.15772968065136\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 201.70094699561687\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 201.70094699561687\nScore: 919.507936924382\nmax_value 252.1261837445211\nScore: 919.507936924382\nmax_value 315.15772968065136\nScore: 919.507936924382\nmax_value 396.1261837445211\nScore: 919.9459129505951\nmax_value 396.1261837445211\nScore: 919.9459129505951\nmax_value 396.1261837445211\nScore: 919.9459129505951\nmax_value 252.1261837445211\nScore: 919.9459129505951\nmax_value 252.1261837445211\nScore: 919.9459129505951\nmax_value 252.1261837445211\nScore: 919.9459129505951\nmax_value 201.70094699561687\nScore: 919.9459129505951\nmax_value 252.1261837445211\nScore: 919.9459129505951\nmax_value 201.70094699561687\nScore: 919.9459129505951\nmax_value 252.1261837445211\nScore: 919.9459129505951\nmax_value 396.1261837445211\nScore: 919.9459129505951\nmax_value 201.70094699561687\nScore: 919.9459129505951\nmax_value 252.1261837445211\nScore: 919.9459129505951\nmax_value 316.9009469956169\nScore: 920.2962937715658\nmax_value 252.1261837445211\nScore: 920.2962937715658\nmax_value 252.1261837445211\nScore: 920.2962937715658\nmax_value 252.1261837445211\nScore: 920.2962937715658\nmax_value 396.1261837445211\nScore: 920.2962937715658\nmax_value 396.1261837445211\nScore: 920.2962937715658\nmax_value 316.9009469956169\nScore: 920.2962937715658\nmax_value 252.1261837445211\nScore: 920.2962937715658\nmax_value 252.1261837445211\nScore: 920.2962937715658\nmax_value 316.9009469956169\nScore: 920.6466745925363\nmax_value 396.1261837445211\nScore: 920.6466745925363\nmax_value 252.1261837445211\nScore: 920.6466745925363\nmax_value 396.1261837445211\nScore: 920.6466745925363\nmax_value 316.9009469956169\nScore: 920.6466745925363\nmax_value 252.1261837445211\nScore: 920.6466745925363\nmax_value 253.52075759649352\nScore: 920.9269792493128\nmax_value 396.1261837445211\nScore: 920.9269792493128\nmax_value 396.1261837445211\nScore: 920.9269792493128\nmax_value 201.70094699561687\nScore: 920.9269792493128\nmax_value 316.9009469956169\nScore: 920.9269792493128\nmax_value 202.81660607719482\nScore: 921.151222974734\nmax_value 316.9009469956169\nScore: 921.151222974734\nmax_value 252.1261837445211\nScore: 921.151222974734\nmax_value 253.52075759649352\nScore: 921.4315276315103\nmax_value 253.52075759649352\nScore: 921.4315276315103\nmax_value 252.1261837445211\nScore: 921.4315276315103\nmax_value 252.1261837445211\nScore: 921.4315276315103\nmax_value 252.1261837445211\nScore: 921.4315276315103\nmax_value 252.1261837445211\nScore: 921.4315276315103\nmax_value 252.1261837445211\nScore: 921.4315276315103\nmax_value 316.9009469956169\nScore: 921.4315276315103\nmax_value 202.81660607719482\nScore: 921.4315276315103\nmax_value 252.1261837445211\nScore: 921.4315276315103\nmax_value 316.9009469956169\nScore: 921.4315276315103\nmax_value 316.9009469956169\nScore: 921.4315276315103\nmax_value 316.9009469956169\nScore: 921.4315276315103\nmax_value 498.01660607719487\nScore: 920.6136722226626\nmax_value 252.1261837445211\nScore: 920.6136722226626\nmax_value 498.4132848617559\nScore: 920.5341550917176\nmax_value 201.70094699561687\nScore: 920.5341550917176\nmax_value 253.52075759649352\nScore: 920.5341550917176\nmax_value 252.1261837445211\nScore: 920.5341550917176\nmax_value 252.1261837445211\nScore: 920.5341550917176\nmax_value 316.9009469956169\nScore: 920.5341550917176\nmax_value 253.52075759649352\nScore: 920.5341550917176\nmax_value 202.81660607719482\nScore: 920.5341550917176\nmax_value 253.52075759649352\nScore: 920.5341550917176\nmax_value 252.1261837445211\nScore: 920.5341550917176\nmax_value 252.1261837445211\nScore: 920.5341550917176\nmax_value 201.70094699561687\nScore: 920.5341550917176\nmax_value 253.52075759649352\nScore: 920.5341550917176\nmax_value 253.52075759649352\nScore: 920.5341550917176\nmax_value 202.81660607719482\nScore: 920.5341550917176\nmax_value 316.9009469956169\nScore: 920.5341550917176\nmax_value 252.1261837445211\nScore: 920.5341550917176\nmax_value 202.81660607719482\nScore: 920.5341550917176\nmax_value 252.1261837445211\nScore: 920.5341550917176\nmax_value 498.4132848617559\nScore: 920.6137434166296\nmax_value 316.9009469956169\nScore: 920.6137434166296\nmax_value 316.9009469956169\nScore: 920.6137434166296\nmax_value 252.1261837445211\nScore: 920.6137434166296\nmax_value 252.1261837445211\nScore: 920.6137434166296\nmax_value 253.52075759649352\nScore: 920.6137434166296\nmax_value 201.70094699561687\nScore: 920.6137434166296\nmax_value 252.1261837445211\nScore: 920.6137434166296\nmax_value 398.7306278894048\nScore: 921.1362905123809\nmax_value 318.98450231152384\nScore: 921.554328188982\nmax_value 318.98450231152384\nScore: 921.972365865583\nmax_value 498.7306278894048\nScore: 921.4493436474943\nmax_value 255.18760184921908\nScore: 921.7835609903873\nmax_value 252.1261837445211\nScore: 921.7835609903873\nmax_value 498.98450231152384\nScore: 921.365452159979\nmax_value 255.18760184921908\nScore: 921.365452159979\nmax_value 255.18760184921908\nScore: 921.6994994590441\nmax_value 204.15008147937527\nScore: 921.9667372982963\nmax_value 252.1261837445211\nScore: 921.9667372982963\nmax_value 255.18760184921908\nScore: 921.9667372982963\nmax_value 201.70094699561687\nScore: 921.9667372982963\nTrained Q matrix:\n[[  0.          51.14138829   0.           0.           0.\n    0.           0.           0.        ]\n [ 40.91311063   0.          63.92673537   0.           0.\n    0.           0.           0.        ]\n [  0.          51.14138829   0.          79.90841921   0.\n   46.67685163   0.           0.        ]\n [  0.           0.          63.92673537   0.           0.\n    0.           0.          99.88552401]\n [  0.           0.           0.           0.           0.\n   50.5278586    0.           0.        ]\n [  0.           0.          63.15982325   0.          40.42228688\n    0.          40.42228688   0.        ]\n [  0.           0.           0.           0.           0.\n   50.5278586    0.           0.        ]\n [  0.           0.           0.          79.3864703    0.\n    0.           0.         100.        ]]\n"}]},{"cell_type":"code","source":"current_state = 0\nsteps = [current_state]\n\nwhile current_state != 7:\n\n    next_step_index = np.where(Q[current_state,]\n        == np.max(Q[current_state,]))[1]\n\n    if next_step_index.shape[0] > 1:\n        next_step_index = int(np.random.choice(next_step_index, size = 1))\n    else:\n        next_step_index = int(next_step_index)\n\n    steps.append(next_step_index)\n    current_state = next_step_index\n\nprint(f\"Most efficient path: {steps}\")\nplt.plot(scores)\nplt.show()","metadata":{"id":"A5WGCzBIz0Ck","colab":{"base_uri":"https://localhost:8080/","height":282},"outputId":"4237b2ca-8353-4754-b207-6be99f5d2fd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Most efficient path: [0, 1, 2, 3, 7]\n"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"\n\n#Shortest Path Using Deep Reinforcement Learning\n","metadata":{"id":"6_guuzaKXGfr"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"vVvF_ZPBvMr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maze = np.array([\n    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n    [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n    [ 1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.],\n    [ 1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.]\n])","metadata":{"id":"DU95wSQX1zbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visited_mark = 0.8  # Cells visited by the rat will be painted by grayscale value 0.8\nrat_mark = 0.5      # The current rat cell will be painteg by grayscale value 0.5\nLEFT, UP, RIGHT, DOWN = 0, 1, 2, 3\n\n# Actions memo\nactions_dict = {\n    LEFT: 'left',\n    UP: 'up',\n    RIGHT: 'right',\n    DOWN: 'down',\n}\n\nnum_actions = len(actions_dict)\n\nepsilon = 0.1   # Exploration factor","metadata":{"id":"WdhDZ7re2AYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# maze is a 2d Numpy array of floats between 0.0 to 1.0\n# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n# rat = (row, col) initial rat position (defaults to (0,0))\n\nclass Qmaze(object):\n    def __init__(self, maze, rat=(0,0)):\n        self._maze = np.array(maze)\n        nrows, ncols = self._maze.shape\n        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n        self.free_cells.remove(self.target)\n        if self._maze[self.target] == 0.0:\n            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n        if not rat in self.free_cells:\n            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n        self.reset(rat)\n\n    def reset(self, rat):\n        self.rat = rat\n        self.maze = np.copy(self._maze)\n        nrows, ncols = self.maze.shape\n        row, col = rat\n        self.maze[row, col] = rat_mark\n        self.state = (row, col, 'start')\n        self.min_reward = -0.5 * self.maze.size\n        self.total_reward = 0\n        self.visited = set()\n\n    def update_state(self, action):\n        nrows, ncols = self.maze.shape\n        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n\n        if self.maze[rat_row, rat_col] > 0.0:\n            self.visited.add((rat_row, rat_col))  # mark visited cell\n\n        valid_actions = self.valid_actions()\n                \n        if not valid_actions:\n            nmode = 'blocked'\n        elif action in valid_actions:\n            nmode = 'valid'\n            if action == LEFT:\n                ncol -= 1\n            elif action == UP:\n                nrow -= 1\n            if action == RIGHT:\n                ncol += 1\n            elif action == DOWN:\n                nrow += 1\n        else:                  # invalid action, no change in rat position\n            mode = 'invalid'\n\n        # new state\n        self.state = (nrow, ncol, nmode)\n\n    def get_reward(self):\n        rat_row, rat_col, mode = self.state\n        nrows, ncols = self.maze.shape\n        if rat_row == nrows-1 and rat_col == ncols-1:\n            return 1.0\n        if mode == 'blocked':\n            return self.min_reward - 1\n        if (rat_row, rat_col) in self.visited:\n            return -0.25\n        if mode == 'invalid':\n            return -0.75\n        if mode == 'valid':\n            return -0.04\n\n    def act(self, action):\n        self.update_state(action)\n        reward = self.get_reward()\n        self.total_reward += reward\n        status = self.game_status()\n        envstate = self.observe()\n        return envstate, reward, status\n\n    def observe(self):\n        canvas = self.draw_env()\n        envstate = canvas.reshape((1, -1))\n        return envstate\n\n    def draw_env(self):\n        canvas = np.copy(self.maze)\n        nrows, ncols = self.maze.shape\n        # clear all visual marks\n        for r in range(nrows):\n            for c in range(ncols):\n                if canvas[r,c] > 0.0:\n                    canvas[r,c] = 1.0\n        # draw the rat\n        row, col, valid = self.state\n        canvas[row, col] = rat_mark\n        return canvas\n\n    def game_status(self):\n        if self.total_reward < self.min_reward:\n            return 'lose'\n        rat_row, rat_col, mode = self.state\n        nrows, ncols = self.maze.shape\n        if rat_row == nrows-1 and rat_col == ncols-1:\n            return 'win'\n\n        return 'not_over'\n\n    def valid_actions(self, cell=None):\n        if cell is None:\n            row, col, mode = self.state\n        else:\n            row, col = cell\n        actions = [0, 1, 2, 3]\n        nrows, ncols = self.maze.shape\n        if row == 0:\n            actions.remove(1)\n        elif row == nrows-1:\n            actions.remove(3)\n\n        if col == 0:\n            actions.remove(0)\n        elif col == ncols-1:\n            actions.remove(2)\n\n        if row>0 and self.maze[row-1,col] == 0.0:\n            actions.remove(1)\n        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n            actions.remove(3)\n\n        if col>0 and self.maze[row,col-1] == 0.0:\n            actions.remove(0)\n        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n            actions.remove(2)\n\n        return actions","metadata":{"id":"vN6-J1Nj47sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(qmaze):\n    plt.grid('on')\n    nrows, ncols = qmaze.maze.shape\n    ax = plt.gca()\n    ax.set_xticks(np.arange(0.5, nrows, 1))\n    ax.set_yticks(np.arange(0.5, ncols, 1))\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    canvas = np.copy(qmaze.maze)\n    for row,col in qmaze.visited:\n        canvas[row,col] = 0.6\n    rat_row, rat_col, _ = qmaze.state\n    canvas[rat_row, rat_col] = 0.3   # rat cell\n    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n    return img","metadata":{"id":"o2JPntLyUUiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maze = [\n    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n]","metadata":{"id":"vnxJGfMeUWQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qmaze = Qmaze(maze)\ncanvas, reward, game_over = qmaze.act(DOWN)\nprint(\"reward=\", reward)\nshow(qmaze)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"wL5cNliVUYVr","outputId":"52ab811a-5aaa-432d-87b1-a9626f5dc382"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"reward= -0.04\n"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fd01d782890>"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"qmaze.act(DOWN)  # move down\nqmaze.act(RIGHT)  # move right\nqmaze.act(RIGHT)  # move right\nqmaze.act(RIGHT)  # move right\nqmaze.act(UP)  # move up\nshow(qmaze)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"qV55o89eUaCG","outputId":"45bf99bc-312f-4bb4-c400-f95cdd57e075"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fd01d259c50>"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"def play_game(model, qmaze, rat_cell):\n    qmaze.reset(rat_cell)\n    envstate = qmaze.observe()\n    while True:\n        prev_envstate = envstate\n        # get next action\n        q = model.predict(prev_envstate)\n        action = np.argmax(q[0])\n\n        # apply action, get rewards and new state\n        envstate, reward, game_status = qmaze.act(action)\n        if game_status == 'win':\n            return True\n        elif game_status == 'lose':\n            return False","metadata":{"id":"gpd-sVBsUcaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def completion_check(model, qmaze):\n    for cell in qmaze.free_cells:\n        if not qmaze.valid_actions(cell):\n            return False\n        if not play_game(model, qmaze, cell):\n            return False\n    return True","metadata":{"id":"1l_UubvWUeKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Experience(object):\n    def __init__(self, model, max_memory=100, discount=0.95):\n        self.model = model\n        self.max_memory = max_memory\n        self.discount = discount\n        self.memory = list()\n        self.num_actions = model.output_shape[-1]\n\n    def remember(self, episode):\n        self.memory.append(episode)\n        if len(self.memory) > self.max_memory:\n            del self.memory[0]\n\n    def predict(self, envstate):\n        return self.model.predict(envstate)[0]\n\n    def get_data(self, data_size=10):\n        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n        mem_size = len(self.memory)\n        data_size = min(mem_size, data_size)\n        inputs = np.zeros((data_size, env_size))\n        targets = np.zeros((data_size, self.num_actions))\n        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n            envstate, action, reward, envstate_next, game_over = self.memory[j]\n            inputs[i] = envstate\n            # There should be no target values for actions not taken.\n            targets[i] = self.predict(envstate)\n            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n            Q_sa = np.max(self.predict(envstate_next))\n            if game_over:\n                targets[i, action] = reward\n            else:\n                # reward + gamma * max_a' Q(s', a')\n                targets[i, action] = reward + self.discount * Q_sa\n        return inputs, targets","metadata":{"id":"dO8sFmVLUgCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def qtrain(model, maze, **opt):\n    global epsilon\n    n_epoch = opt.get('n_epoch', 15000)\n    max_memory = opt.get('max_memory', 1000)\n    data_size = opt.get('data_size', 50)\n    weights_file = opt.get('weights_file', \"\")\n    name = opt.get('name', 'model')\n    start_time = datetime.datetime.now()\n\n    if weights_file:\n        print(\"loading weights from file: %s\" % (weights_file,))\n        model.load_weights(weights_file)\n\n    qmaze = Qmaze(maze)\n\n    # Initialize experience replay object\n    experience = Experience(model, max_memory=max_memory)\n\n    win_history = []   # history of win/lose game\n    n_free_cells = len(qmaze.free_cells)\n    hsize = qmaze.maze.size//2   # history window size\n    win_rate = 0.0\n    imctr = 1\n\n    for epoch in range(n_epoch):\n        loss = 0.0\n        rat_cell = random.choice(qmaze.free_cells)\n        qmaze.reset(rat_cell)\n        game_over = False\n\n        # get initial envstate (1d flattened canvas)\n        envstate = qmaze.observe()\n\n        n_episodes = 0\n        while not game_over:\n            valid_actions = qmaze.valid_actions()\n            if not valid_actions: break\n            prev_envstate = envstate\n            # Get next action\n            if np.random.rand() < epsilon:\n                action = random.choice(valid_actions)\n            else:\n                action = np.argmax(experience.predict(prev_envstate))\n\n            # Apply action, get reward and new envstate\n            envstate, reward, game_status = qmaze.act(action)\n            if game_status == 'win':\n                win_history.append(1)\n                game_over = True\n            elif game_status == 'lose':\n                win_history.append(0)\n                game_over = True\n            else:\n                game_over = False\n\n            # Store episode (experience)\n            episode = [prev_envstate, action, reward, envstate, game_over]\n            experience.remember(episode)\n            n_episodes += 1\n\n            # Train neural network model\n            inputs, targets = experience.get_data(data_size=data_size)\n            h = model.fit(\n                inputs,\n                targets,\n                epochs=8,\n                batch_size=16,\n                verbose=0,\n            )\n            loss = model.evaluate(inputs, targets, verbose=0)\n\n        if len(win_history) > hsize:\n            win_rate = sum(win_history[-hsize:]) / hsize\n    \n        dt = datetime.datetime.now() - start_time\n        t = format_time(dt.total_seconds())\n        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n        # we simply check if training has exhausted all free cells and if in all\n        # cases the agent won\n        if win_rate > 0.9 : epsilon = 0.05\n        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n            break\n\n    # Save trained model weights and architecture, this will be used by the visualization code\n    h5file = name + \".h5\"\n    json_file = name + \".json\"\n    model.save_weights(h5file, overwrite=True)\n    with open(json_file, \"w\") as outfile:\n        json.dump(model.to_json(), outfile)\n    end_time = datetime.datetime.now()\n    dt = datetime.datetime.now() - start_time\n    seconds = dt.total_seconds()\n    t = format_time(seconds)\n    print('files: %s, %s' % (h5file, json_file))\n    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n    return seconds\n\n# This is a small utility for printing readable time strings:\ndef format_time(seconds):\n    if seconds < 400:\n        s = float(seconds)\n        return \"%.1f seconds\" % (s,)\n    elif seconds < 4000:\n        m = seconds / 60.0\n        return \"%.2f minutes\" % (m,)\n    else:\n        h = seconds / 3600.0\n        return \"%.2f hours\" % (h,)","metadata":{"id":"Ncvxxtw3UjIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(maze, lr=0.001):\n    model = Sequential()\n    model.add(Dense(maze.size, input_shape=(maze.size,)))\n    model.add(PReLU())\n    model.add(Dense(maze.size))\n    model.add(PReLU())\n    model.add(Dense(num_actions))\n    model.compile(optimizer='adam', loss='mse')\n    return model","metadata":{"id":"powNWat5UnRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maze =  np.array([\n    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n])\n\nqmaze = Qmaze(maze)\nshow(qmaze)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"q2AqyhMzUpP9","outputId":"0b0d645c-fdce-47bc-ffb3-5bcd2b7a651e"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fd01d1dda10>"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"model = build_model(maze)\nqtrain(model, maze, epochs=100, max_memory=8*maze.size, data_size=32)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yoYfVz0eUsK5","outputId":"5c546892-fbde-429a-efd4-b0b757fd4f6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 000/14999 | Loss: 0.0023 | Episodes: 106 | Win count: 0 | Win rate: 0.000 | time: 302.9 seconds\nEpoch: 001/14999 | Loss: 0.0565 | Episodes: 111 | Win count: 0 | Win rate: 0.000 | time: 11.24 minutes\nEpoch: 002/14999 | Loss: 0.0019 | Episodes: 104 | Win count: 0 | Win rate: 0.000 | time: 17.00 minutes\nEpoch: 003/14999 | Loss: 0.0034 | Episodes: 29 | Win count: 1 | Win rate: 0.000 | time: 18.56 minutes\nEpoch: 004/14999 | Loss: 0.0485 | Episodes: 104 | Win count: 1 | Win rate: 0.000 | time: 24.11 minutes\nEpoch: 005/14999 | Loss: 0.0041 | Episodes: 106 | Win count: 1 | Win rate: 0.000 | time: 29.78 minutes\nEpoch: 006/14999 | Loss: 0.0135 | Episodes: 106 | Win count: 1 | Win rate: 0.000 | time: 35.53 minutes\nEpoch: 007/14999 | Loss: 0.0029 | Episodes: 107 | Win count: 1 | Win rate: 0.000 | time: 41.35 minutes\nEpoch: 008/14999 | Loss: 0.0016 | Episodes: 104 | Win count: 1 | Win rate: 0.000 | time: 46.99 minutes\nEpoch: 009/14999 | Loss: 0.0013 | Episodes: 104 | Win count: 1 | Win rate: 0.000 | time: 52.66 minutes\nEpoch: 010/14999 | Loss: 0.0028 | Episodes: 101 | Win count: 1 | Win rate: 0.000 | time: 58.26 minutes\nEpoch: 011/14999 | Loss: 0.0010 | Episodes: 104 | Win count: 1 | Win rate: 0.000 | time: 63.95 minutes\nEpoch: 012/14999 | Loss: 0.0026 | Episodes: 103 | Win count: 1 | Win rate: 0.000 | time: 1.16 hours\nEpoch: 013/14999 | Loss: 0.0022 | Episodes: 102 | Win count: 1 | Win rate: 0.000 | time: 1.25 hours\nEpoch: 014/14999 | Loss: 0.0014 | Episodes: 65 | Win count: 2 | Win rate: 0.000 | time: 1.31 hours\nEpoch: 015/14999 | Loss: 0.0021 | Episodes: 106 | Win count: 2 | Win rate: 0.000 | time: 1.41 hours\nEpoch: 016/14999 | Loss: 0.0015 | Episodes: 111 | Win count: 2 | Win rate: 0.000 | time: 1.51 hours\nEpoch: 017/14999 | Loss: 0.0023 | Episodes: 109 | Win count: 2 | Win rate: 0.000 | time: 1.60 hours\nEpoch: 018/14999 | Loss: 0.0015 | Episodes: 55 | Win count: 3 | Win rate: 0.000 | time: 1.65 hours\nEpoch: 019/14999 | Loss: 0.0102 | Episodes: 104 | Win count: 3 | Win rate: 0.000 | time: 1.75 hours\nEpoch: 020/14999 | Loss: 0.0221 | Episodes: 102 | Win count: 3 | Win rate: 0.000 | time: 1.84 hours\nEpoch: 021/14999 | Loss: 0.0357 | Episodes: 108 | Win count: 3 | Win rate: 0.000 | time: 1.93 hours\nEpoch: 022/14999 | Loss: 0.0013 | Episodes: 105 | Win count: 3 | Win rate: 0.000 | time: 2.02 hours\nEpoch: 023/14999 | Loss: 0.0014 | Episodes: 106 | Win count: 3 | Win rate: 0.000 | time: 2.12 hours\nEpoch: 024/14999 | Loss: 0.0032 | Episodes: 23 | Win count: 4 | Win rate: 0.167 | time: 2.14 hours\nEpoch: 025/14999 | Loss: 0.0055 | Episodes: 104 | Win count: 4 | Win rate: 0.167 | time: 2.23 hours\nEpoch: 026/14999 | Loss: 0.0012 | Episodes: 103 | Win count: 4 | Win rate: 0.167 | time: 2.33 hours\nEpoch: 027/14999 | Loss: 0.0012 | Episodes: 3 | Win count: 5 | Win rate: 0.167 | time: 2.33 hours\nEpoch: 028/14999 | Loss: 0.1357 | Episodes: 107 | Win count: 5 | Win rate: 0.167 | time: 2.42 hours\nEpoch: 029/14999 | Loss: 0.0017 | Episodes: 10 | Win count: 6 | Win rate: 0.208 | time: 2.43 hours\nEpoch: 030/14999 | Loss: 0.0429 | Episodes: 103 | Win count: 7 | Win rate: 0.250 | time: 2.52 hours\nEpoch: 031/14999 | Loss: 0.0033 | Episodes: 4 | Win count: 8 | Win rate: 0.292 | time: 2.52 hours\nEpoch: 032/14999 | Loss: 0.0034 | Episodes: 104 | Win count: 8 | Win rate: 0.292 | time: 2.62 hours\nEpoch: 033/14999 | Loss: 0.0022 | Episodes: 103 | Win count: 8 | Win rate: 0.292 | time: 2.71 hours\nEpoch: 034/14999 | Loss: 0.0020 | Episodes: 107 | Win count: 8 | Win rate: 0.292 | time: 2.80 hours\nEpoch: 035/14999 | Loss: 0.0016 | Episodes: 104 | Win count: 8 | Win rate: 0.292 | time: 2.89 hours\nEpoch: 036/14999 | Loss: 0.0005 | Episodes: 68 | Win count: 9 | Win rate: 0.333 | time: 2.95 hours\nEpoch: 037/14999 | Loss: 0.0018 | Episodes: 103 | Win count: 9 | Win rate: 0.333 | time: 3.04 hours\nEpoch: 038/14999 | Loss: 0.0010 | Episodes: 2 | Win count: 10 | Win rate: 0.333 | time: 3.04 hours\nEpoch: 039/14999 | Loss: 0.0016 | Episodes: 104 | Win count: 10 | Win rate: 0.333 | time: 3.14 hours\nEpoch: 040/14999 | Loss: 0.0017 | Episodes: 3 | Win count: 11 | Win rate: 0.375 | time: 3.14 hours\nEpoch: 041/14999 | Loss: 0.0023 | Episodes: 62 | Win count: 12 | Win rate: 0.417 | time: 3.20 hours\nEpoch: 042/14999 | Loss: 0.0149 | Episodes: 104 | Win count: 12 | Win rate: 0.375 | time: 3.29 hours\nEpoch: 043/14999 | Loss: 0.0029 | Episodes: 3 | Win count: 13 | Win rate: 0.417 | time: 3.30 hours\nEpoch: 044/14999 | Loss: 0.0050 | Episodes: 14 | Win count: 14 | Win rate: 0.458 | time: 3.31 hours\nEpoch: 045/14999 | Loss: 0.0072 | Episodes: 61 | Win count: 15 | Win rate: 0.500 | time: 3.37 hours\nEpoch: 046/14999 | Loss: 0.0024 | Episodes: 15 | Win count: 16 | Win rate: 0.542 | time: 3.38 hours\nEpoch: 047/14999 | Loss: 0.0109 | Episodes: 18 | Win count: 17 | Win rate: 0.583 | time: 3.39 hours\nEpoch: 048/14999 | Loss: 0.0019 | Episodes: 104 | Win count: 17 | Win rate: 0.542 | time: 3.49 hours\nEpoch: 049/14999 | Loss: 0.0039 | Episodes: 53 | Win count: 18 | Win rate: 0.583 | time: 3.54 hours\nEpoch: 050/14999 | Loss: 0.0025 | Episodes: 15 | Win count: 19 | Win rate: 0.625 | time: 3.55 hours\nEpoch: 051/14999 | Loss: 0.0395 | Episodes: 10 | Win count: 20 | Win rate: 0.625 | time: 3.56 hours\nEpoch: 052/14999 | Loss: 0.0013 | Episodes: 25 | Win count: 21 | Win rate: 0.667 | time: 3.58 hours\nEpoch: 053/14999 | Loss: 0.0015 | Episodes: 23 | Win count: 22 | Win rate: 0.667 | time: 3.60 hours\nEpoch: 054/14999 | Loss: 0.0046 | Episodes: 117 | Win count: 23 | Win rate: 0.667 | time: 3.70 hours\nEpoch: 055/14999 | Loss: 0.0018 | Episodes: 37 | Win count: 24 | Win rate: 0.667 | time: 3.73 hours\nEpoch: 056/14999 | Loss: 0.0008 | Episodes: 11 | Win count: 25 | Win rate: 0.708 | time: 3.74 hours\nEpoch: 057/14999 | Loss: 0.0005 | Episodes: 29 | Win count: 26 | Win rate: 0.750 | time: 3.77 hours\nEpoch: 058/14999 | Loss: 0.0013 | Episodes: 33 | Win count: 27 | Win rate: 0.792 | time: 3.80 hours\nEpoch: 059/14999 | Loss: 0.0011 | Episodes: 18 | Win count: 28 | Win rate: 0.833 | time: 3.82 hours\n"}]},{"cell_type":"markdown","source":"#Reinforcement Learning","metadata":{"id":"9QaURe3mc4CB"}},{"cell_type":"markdown","source":"## MountainCar-v0","metadata":{"id":"n2bi4E5YfkzP"}},{"cell_type":"code","source":"class State:\n  def __init__(self, position_tile, velocity_tile):\n    self.position_tile = position_tile\n    self.velocity_tile = velocity_tile\n\n  def __eq__(self, other):\n    if isinstance(other, State):\n      return (self.position_tile == other.position_tile and\n              self.velocity_tile == other.velocity_tile)\n    else:\n      return False\n\n  def __hash__(self):\n    return hash((self.position_tile, self.velocity_tile))","metadata":{"id":"6vdvl0w4f4QG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ACTIONS_COUNT = 3\neps_greedy = 0\n\ndef find_best_action_quality(Q, state):\n  best_action, best_q = None, None\n  for action in range(ACTIONS_COUNT):\n    cur_q = Q[(state, action)]\n    if best_q is None or best_q < cur_q:\n      best_action, best_q = action, cur_q\n  return best_action, best_q\n\n\ndef choose_eps_greedy_action(Q, state):\n  best_action, best_q = find_best_action_quality(Q, state)\n  best_count = 0\n  for action in range(ACTIONS_COUNT):\n    if Q[(state, action)] == best_q:\n        best_count += 1\n  p = []\n  for action in range(ACTIONS_COUNT):\n    prob = eps_greedy / ACTIONS_COUNT\n    if Q[(state, action)] == best_q:\n      prob += (1 - eps_greedy) / best_count\n    p.append(prob)\n  return np.random.choice(ACTIONS_COUNT, 1, p=p)[0]\n\n\ndef map_observation_to_state(observation, position_grid, velocity_grid):\n  return State(int(round(observation[0] / position_grid)),\n                int(round(observation[1] / velocity_grid)))","metadata":{"id":"X-DIriV3gFTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_episodes = 5000\ntimesteps_limit = 200  # limit from OpenAI gym docs\ngamma = 0.9\n\ndef evaluate_parameters(alpha, position_grid, velocity_grid):\n  env = gym.make('MountainCar-v0')\n  env.seed(0)\n  np.random.seed(0)\n  cumulative_completion = []\n  completed = 0\n  Q = defaultdict(lambda: 0.0)\n  for episode in range(training_episodes):\n    observation = env.reset()\n    state = map_observation_to_state(\n      observation, position_grid, velocity_grid)\n    action = choose_eps_greedy_action(Q, state)\n    for timestep in range(timesteps_limit):\n      observation, reward, done, info = env.step(action)\n      to_state = map_observation_to_state(\n        observation, position_grid, velocity_grid)\n      next_action = choose_eps_greedy_action(Q, to_state)\n      Q[(state, action)] += alpha * (reward +\n                                      gamma * Q[(to_state, action)] -\n                                      Q[(state, action)])\n      action, state = next_action, to_state\n      if done:\n        if timestep != timesteps_limit - 1:\n          completed += 1\n        cumulative_completion.append(completed)\n        break\n  env.close()\n  return cumulative_completion","metadata":{"id":"bvMjBIwOgG1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_and_plot_parameters(alpha, position_grid, velocity_grid, color):\n  cumulative_completion = evaluate_parameters(alpha, position_grid, velocity_grid)\n  title = f'alpha={alpha} pos_grid={position_grid} vel_grid={velocity_grid}'\n  print(f'Evaluated {title}')\n  line, = plt.plot(\n      np.arange(1, training_episodes + 1),\n      cumulative_completion,\n      c=next(color),\n      label=title)\n  return line","metadata":{"id":"8sMTRo1UgIYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color = iter(cm.rainbow(np.linspace(0, 1, 5)))\nhandles = []\n\nalpha, position_grid, velocity_grid = 0.25, 0.05, 0.01\nhandles.append(evaluate_and_plot_parameters(\n  alpha, position_grid, velocity_grid, color\n))\n\nplt.xlabel('episodes experienced')\nplt.ylabel('overall episodes completed')\nplt.legend(handles=handles)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"jzfSsvw4gJmk","outputId":"98d65df6-aeed-4e85-b655-db741e25a3bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Evaluated alpha=0.25 pos_grid=0.05 vel_grid=0.01\n"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f269f4c4b10>"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Roulette","metadata":{"id":"MMk82luBgPLW"}},{"cell_type":"code","source":"class Model(object):\n\tdef __init__(self, *args):\n\t\tif not args is None:\n\t\t\tself.env = args[0]\n\t\t\tself.Q = args[1]\n\t\t\tself.alpha = args[2]\n\t\t\tself.gamma = args[3]\n\t\t\tself.epsilon = args[4]\n\t\t\tself.n_episodes = args[5]\n\t\t\tself.verbose = args[6]\n\t\t\tself.record_training = args[7]\n\t\t\tself.checkpoint = self.n_episodes * 0.1\n\t\telse:\n\t\t\tprint('Invalid arguments.')\n\n\tdef eps_greedy(self, obs):\n\t\tif np.random.uniform() < self.epsilon:\n\t\t\treturn np.random.randint(self.env.action_space.n)\n\t\telse:\n\t\t\taction_values = [self.Q[obs, a] for a in \n\t\t\t\t\t\t\t range(self.env.action_space.n)]\n\t\t\tgreedy_idx = np.argwhere(action_values == np.max(action_values))\n\t\t\tgreedy_act_idx = np.random.choice(greedy_idx.flatten())\n\t\t\treturn greedy_act_idx\n\n\tdef greedy_action(self, obs):\n\t\taction_values = [self.Q[obs, a] for a in \n\t\t\t\t\t\t range(self.env.action_space.n)]\n\t\tgreedy_idx = np.argmax(action_values)\n\t\treturn greedy_idx\n\n\tdef train(self, idx=None, q=None):\n\t\tif self.record_training:\n\t\t\tself.all_rewards = []\n\n\t\tfor episode in range(self.n_episodes):\n\t\t\tdone = False\n\t\t\tobs = self.env.reset()\n\t\t\tif self.record_training:\n\t\t\t\tepisode_reward = 0\n\t\t\ta = self.eps_greedy(obs)\n\n\t\t\twhile not done:\n\t\t\t\tobs_prime, reward, done, info = self.env.step(a)\n\t\t\t\ta_prime = self.eps_greedy(obs_prime)\n\t\t\t\tself.Q[obs,a] += self.alpha * (reward + self.gamma*self.Q[obs_prime, a] -\n\t\t\t\t\t\t\t\t\t\t\t\t   self.Q[obs, a])\n\t\t\t\tif self.record_training:\n\t\t\t\t\tepisode_reward += reward\n\t\t\t\tobs = obs_prime\n\t\t\t\ta = a_prime\n\t\t\t\t\n\t\t\t\t\n\t\t\tif self.record_training:\n\t\t\t\tself.all_rewards.append(episode_reward)\n\t\t\tif self.verbose and episode % self.checkpoint == 0:\n\t\t\t\tif not idx is None:\n\t\t\t\t\tprint(f'Agent: {idx} Episode: {episode}')\n\t\t\t\telse:\n\t\t\t\t\tprint(f'Episode: {episode}')\n\t\tif not q is None:\n\t\t\tq.put(self)\n\t\tif not idx is None:\n\t\t\tprint(f'Agent: {idx} - Training complete.')\n\t\telse:\n\t\t\tprint('Training complete.')","metadata":{"id":"DzcSQ9oQgR7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize environment, hyperparameters and action value function.\ngamma = 1\nalpha = 0.1\nepsilon = 0.1\nn_epsiodes = 10000\nenv = gym.make('Roulette-v0')\nQ = dict.fromkeys(product([0], range(38)), 0.0)\n\n# Create and train agent.\nagent = Model(env, Q, alpha, gamma, epsilon, n_epsiodes, True, False)\nagent.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqDTzUNpgTzP","outputId":"c27b4844-6a68-4861-8010-d9ed15beeedc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Episode: 0\nEpisode: 1000\nEpisode: 2000\nEpisode: 3000\nEpisode: 4000\nEpisode: 5000\nEpisode: 6000\nEpisode: 7000\nEpisode: 8000\nEpisode: 9000\nTraining complete.\n"}]},{"cell_type":"code","source":"action_values = np.array([i for i in agent.Q.values()])\nplt.bar(range(len(action_values)), action_values)\nplt.xticks(range(len(action_values)))\nplt.tick_params(axis='x', which='major', labelsize=9)\nplt.xlabel('Agent bet: ')\nplt.ylabel('Action value: ')\nplt.title('Learned action value function for Roulette-v0:')\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"RaKte-7mgVVE","outputId":"357147c7-ea5f-41cc-d404-4e296c50d3cb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgcVb3G8e9LAoQlCYQ9ARIQEFlCkAhyZROisnlBiCwiyiZwvbg84sIqIIvgzgURgyiKEEARREA2ARFBICA7iizBsGlYkwASSH73j3OaqVS6Z3oy09MzqffzPPNMd5+qU6eqT9WvzjlV1YoIzMysehZpdwHMzKw9HADMzCrKAcDMrKIcAMzMKsoBwMysohwAzMwqygFgISXpPEkn9cFyzpZ0bKuX0x2Spkqa0IJ8l5D0O0mvSvpVb+ffxbIfkrRNC/JdSdItkmZK+m5v59+bJN0s6aB2l2Nh4gDQhFYdUAYaSftJurX4WUQcGhEntqtMfWwisBKwXER8vFULqRe8I2L9iLi5BYs7GHgBGBYRh/c0s1xH5kiaJWmGpPsk7dzzYna7HCFprcL7bSQ93cLljZF0k6TXJf1toBwvHAD6CUmD2l0G69Jo4NGIeLvdBelFo4GHYwHuCJU0uEHS7RGxNLAMcBZwkaRlelDGgWAy8FdgOeBo4NeSVmhvkZoQEf7r4g+YCkyo8/kiwBHA48CLwCXAiEL6r4DngVeBW4D1C2nnAT8CrgZeAybk5XwZuD/PczEwpDDPzsC9wCvAbcDYQtrGwD3AzDzfRcBJDdbnXcCNucwvABcAyxTSVwN+A0zP05wJvAf4DzAHmAW8UliPkwrzfgZ4DHgJuAIYWUgL4FDgH3kdfgioTvlGAm+UtuXGuayLNlH+d76vOuXbBni6tKxL87o+CXy+wTY7AZgNvJXX/0DgeOCXhWnG5HUcnN/fDJwI/Dl/L9cByxem3yJ/j68A04D9SGfkb+VlzQJ+V2edFgd+ADyb/34ALF5cP+Bw4N/Ac8D+DdbpvNKyJjSZ99dI9fr8OnnuB9xaeL9k3ibvy++HA7/I2/sp4BhgkZzWzPY8qJB+APAI8DJwLTA6f35Lnu+1vF6fJtWnufn9rPy9d7r/ltbr98Bhpc/uA3YD1gHeBIYW0v4EHNruY1dXf24B9MzngF2BrUkV6mXSQa3m98DawIqkg/MFpfk/AZwMDAVqXSt7ANsDawBjSTsUkjYGfgocQjrL+DFwhaTFJS0GXA6cD4wgBZ7dOym3gG/mMr+HdMA/Pi9nEHAlaeccA4wCLoqIR0gH79sjYumImO+MTtK2Od89gFVyHheVJtsZeF9etz2Aj5TziYhngdtL6/AJ4NcR8VZn5e8OSYsAvyPtyKOA7YAvSqpXpuOAU4CL8/qf2+RiPgHsT6oDi5ECPJJGk+rHGcAKwDjg3oiYRKon38rL+WidPI8G3p/n2QjYlHQgrVmZdKAdRQpUP5S0bJ112q+0rBuazHsEqeVwcGcrnuvS/qQg81T++IxctjVJ+82n8jTdImkX4CjSAXgF0gF3cl6vrfJkG+X1+jmwA/Bsfr90rmNd7b9Fk4G9C8tfj7QNrgLWB56IiJmF6e/LnyNpC0mvdHcd+0S7I9BA+KNxC+ARYLvC+1VIlX1wnWmXIZ2VDM/vzwN+UWc5nyy8/xZwdn79I+DE0vR/J1XerUhnayqk3UaDFkCdsu0K/DW/3px0dlZvHfajcHZXWI+T8utzSQeTWtrSeXuMye8D2KKQfglwRIMyHQTcmF+LdIa8VVflL39fdNICADYD/lnK60jgZw2WczzznqGW349h/jPWYwrpnwWuKSznsgbLmafMddbpcWDHQtpHgKmF9Xuj+P2RWgLvb2ZZTeQ9m0KrtEEdeZvUqnkrl2WPnDYoz79eYfpDgJu7sT0Pyq9/DxxYmHYR4HU6WgEBrFXve1/A/XcoqUVRy/9k4Kf59b7AX0rTnwyc18z+184/twB6ZjRwmaRXcoR/hNRFspKkQZJOlfS4pBmkHRhg+cL80+rk+Xzh9eukg2htWYfXlpWXtxrpzGUk8Ezkmpc9RQP5yo+LJD2Ty/bLQrlWA56KBevnHllcbkTMIjWtRzWxfmWXAptLWoUU4OaSzvK6Kn93jAZGlrbpUaSB3t7SaH1XIx1sF8Q82zm/Hll4/2Lp++tsO3c37+kR8Z8u8vhLpBbisqRuwC3z58uTuvDK+Y+i+0YDpxe+t5dIJwrdyauz/ffsPJA9S9JRkc7urwL2yvPuTUeLfhYwrJT3MFK3X7/mANAz04AdImKZwt+QiHiG1PTfhdSvOpx0NgOpktZ0Z+BtGnByaVlLRsRkUj/vKEnFvFfvJK9T8rI3jIhhwCcL5ZoGrN5ggK+r8j5L2qkAkLQUqbvqmS7mm39BES+T+sz3JG3LiwoBrrPyl71G6oeuWbnwehrwZGmbDo2IHZssZmd5d2UaaSyjnm5tZ9J3/Ww3lt2TvJuus/kE4H+AfXMX5gukM+xy/rX60Z3tOQ04pPTdLRERtzUqToM86u6/ka5uq3UXnZKnnwzsLWlzYAhwU/78IWBNSUMLeW+UP+/XHACat6ikIYW/wcDZwMm5PxdJK+S+SUhNxjdJZ8BLkg5aPXEOcKikzZQsJWmnXOluJzW7Py9pUUm7kfpuGxlKOmt5VdIo4CuFtDtJAeXUvIwhkj6Q0/4FrJrHHOqZDOwvaZykxUnrfEdETF3Adb6Q1Ec8Mb9upvxl9wI7ShohaWXgi4W0O4GZkr6Wr/EfJGkDSe9rsnz3AltJWl3ScFK3TrMuACZI2kPSYEnLSRqX0/5F6iNvZDJwTK5vywNfJ7WCekOv5h0RLwE/Ab4eEXNI3X4nSxqa95svFfLvzvY8GzhSUq2ffbik4qW55W34L2C5nG8xj0b7bz1Xk4LXN0hjQXPzOj6ay35c3l8+RhrjurSTvPoFB4DmXU3qz6z9HQ+cTmriXidpJvAXUr8ypCsdniKd3Tyc0xZYREwhXWFzJmmw6jHyAHFEzCYNhu1HagrvSbqKp5ETgPeSrjS6qjht3kk/CqwF/JN01ceeOflG0lnN85JeqFPGG4BjSRX/OdIZ7l7l6brhCtIg+vMRcV8z5a/jfNKA3FRSi+LiQnnnkAalx5GuAHqBdLAaPl8udUTE9Tm/+4G7SYPnTYmIfwI7kq7WeYl0ANkoJ58LrJe7Ji6vM/tJwJS83AdIFxj01k1/rcj7B6QgPJY08Poa8ATpwocLSRc3dGt7RsRlwGmkS0xnAA+SBnprjgd+nrfhHhHxN1JweyJ/NpLO9996y3yTVNcmMO8JCaR6Pp60b54KTIyI6QCStpQ0q9Mt1Caat9vYzMyqwi0AM7OKamsAkLS9pL9LekzSEe0si5lZ1bStCyjfJPIo8CFSP/NdwN4R8XBbCmRmVjHtbAFsCjwWEU/kQcyLSJdNmplZH2j0MKe+MIp5b4R6mjoj8JIOJt9yvtRSS22y7rrrLtDCHnjm1YZpG44aPuDTofE6Or216bVpFvZ06L/fwcKS3ip33333CxEx38Pp2tkFNBHYPiIOyu/3BTaLiMMazTN+/PiYMmXKAi1vzBFXNUybeupOAz4dGq+j01ubXptmYU+H/vsdLCzprSLp7ogYX/68nV1Az5Buh69ZlQW4Y9TMzBZMOwPAXcDaktbId5buRbopw8zM+kDbxgAi4m1Jh5Ge4z2I9GS9fv/sDDOzhUU7B4GJiKtJj1jo93raR9fqPr6BUgbrv1w/qsd3ApuZVZQDgJlZRTkAmJlVlAOAmVlFtXUQ2Dp4AK7/83dkXdWBgVZH3AIwM6soBwAzs4pyADAzqyiPAfSSgdb3V8/CsA7Wv/W0D911tHe5BWBmVlEOAGZmFeUAYGZWUQ4AZmYV5UFgs17S7ifGeoDUusstADOzinIAMDOrKAcAM7OK8hiADRgDvY97oJcfFo51sA5uAZiZVZQDgJlZRTkAmJlVlAOAmVlFeRB4gOgPg2+tLkN/WMeFmbevlbkFYGZWUQ4AZmYV5QBgZlZRDgBmZhXlQWBbaPhpmtbf9bc65haAmVlFOQCYmVWUA4CZWUV5DCDrb31zZtZ93o+7xy0AM7OKcgAwM6soBwAzs4pyADAzqygHADOzimpLAJD0cUkPSZoraXw7ymBmVnXtagE8COwG3NKm5ZuZVV5b7gOIiEcAJLVj8WZmxgC4EUzSwcDBAKuvvnqbS2NmA5lvFJtXywKApBuAleskHR0Rv202n4iYBEwCGD9+fPRS8czMKq9lASAiJrQqbzMz6zlfBmpmVlHtugz0Y5KeBjYHrpJ0bTvKYWZWZe26Cugy4LJ2LNtaxwNsZgOLu4DMzCrKAcDMrKIcAMzMKsoBwMysovr9ncDWvP4+CNvu8rV7+T010Mtv/Y9bAGZmFeUAYGZWUQ4AZmYV5TEAM7OsauMsbgGYmVWUA4CZWUU5AJiZVZQDgJlZRXkQ2Mx6TdUGUQc6twDMzCrKAcDMrKIcAMzMKsoBwMysohwAzMwqygHAzKyiHADMzCrKAcDMrKIcAMzMKsoBwMysohwAzMwqygHAzKyiHADMzCrKAcDMrKIcAMzMKqqpACDpns7em5nZwNNUAIiI93b23szMBp5mWwCjJU3Ir5eQNLS1xTIzs1brMgBI+gzwa+DH+aNVgctbWSgzM2u9ZloA/wt8AJgBEBH/AFZsZaHMzKz1mgkAb0bE7NobSYOBaF2RzMysLzQTAP4o6ShgCUkfAn4F/K61xTIzs1ZrJgAcAUwHHgAOAa4GjmlloczMrPUGdzVBRMwFzsl/vULSt4GPArOBx4H9I+KV3srfzMy61sxVQE9KeqL818PlXg9sEBFjgUeBI3uYn5mZdVOXLQBgfOH1EODjwIieLDQiriu8/QswsSf5mZlZ93XZAoiIFwt/z0TED4CderEMBwC/b5Qo6WBJUyRNmT59ei8u1sys2rpsAUgqPvZhEVKLoJn5bgBWrpN0dET8Nk9zNPA2cEGjfCJiEjAJYPz48b781MyslzTTBfTdwuu3ganAHl3NFBETOkuXtB+wM7BdRPjAbmbWx5q5CuiDvb1QSdsDXwW2jojXezt/MzPrWsMAIOlLnc0YEd/rwXLPBBYHrpcE8JeIOLQH+ZmZWTd11gJo2RM/I2KtVuVtZmbNaRgAIuKEviyImZn1rWau5hkCHAisT7oPAICIOKCF5TIzsxZr5llA55Mu5/wI8EfS7wHMbGWhzMys9ZoJAGtFxLHAaxHxc9JNYJu1tlhmZtZqzQSAt/L/VyRtAAzHPwhjZjbgNXMj2CRJywLHAlcAS+fXZmY2gDUTAH4WEXNI/f9rtrg8ZmbWR5rpAnpS0iRJ2ynftWVmZgNfMwFgXeAG0o/DT5V0pqQtWlssMzNrtWYeB/16RFwSEbsB44BhpO4gMzMbwJppASBpa0lnAXeTbgbr8mmgZmbWvzVzJ/BU4K/AJcBXIuK1VhfKzMxar5mrgMZGxIyWl8TMzPpUM2MAPvibmS2EmhoDMDOzhY8DgJlZRTUzCLw4sDswpjh9RHyjdcUyM7NWa2YQ+LfAq6RLQN9sbXHMzKyvNBMAVo2I7VteEjMz61PNjAHcJmnDlpfEzMz6VDMtgC2A/SQ9SeoCEhARMbalJTMzs5ZqJgDs0PJSmJlZn2vmRrCngGWAj+a/ZfJnZmY2gHUZACR9AbiA9DOQKwK/lPS5VhfMzMxaq5kuoAOBzWoPgZN0GnA7cEYrC2ZmZq3VzFVAAuYU3s/Jn5mZ2QDW1G8CA3dIuiy/3xU4t3VFMjOzvtBlAIiI70m6mXQ5KMD+EfHXlpbKzMxarmEAkDQsImZIGgFMzX+1tBER8VLri2dmZq3SWQvgQmBn0jOAovC58vs1W1guMzNrsYYBICJ2zv/X6LvimJlZX2nmPoA/NPOZmZkNLJ2NAQwBlgSWl7QsHZd+DgNG9UHZzMyshTobAzgE+CIwkjQOUAsAM4AzW1wuMzNrsc7GAE4HTpf0uYjwXb9mZguZZu4EnitpmdobSctK+mwLy2RmZn2gmQDwmYh4pfYmIl4GPtO6IpmZWV9oJgAMkvTOs38kDQIW68lCJZ0o6X5J90q6TtLInuRnZmbd10wAuAa4WNJ2krYDJufPeuLbETE2IsYBVwJf72F+ZmbWTc08DO5rwMHA/+T31wPn9GShETGj8HYp5r3T2MzM+kAzvwg2NyLOjoiJETEReJhe+C0ASSdLmgbsQyctAEkHS5oiacr06dN7ulgzM8ua6QJC0saSviVpKvAN4G9NzHODpAfr/O0CEBFHR8RqpF8bO6xRPhExKSLGR8T4FVZYoamVMjOzrnV2J/A6wN757wXgYkAR8cFmMo6ICU2W4QLgauC4Jqc3M7Ne0FkL4G/AtsDOEbFFvhlsTifTN03S2oW3u9BEi8LMzHpXZ4PAuwF7ATdJuga4iN77KchTJb0bmAs8BRzaS/mamVmTOnsUxOXA5ZKWIp2lfxFYUdKPgMsi4roFXWhE7L6g85qZWe9o5iqg1yLiwoj4KLAq8FfSpaFmZjaANXUVUE1EvJyvytmuVQUyM7O+0a0AYGZmCw8HADOzinIAMDOrKAcAM7OKcgAwM6soBwAzs4pyADAzqygHADOzinIAMDOrKAcAM7OKcgAwM6soBwAzs4pyADAzqygHADOzinIAMDOrKAcAM7OKcgAwM6soBwAzs4pyADAzqygHADOzinIAMDOrKAcAM7OKcgAwM6soBwAzs4pyADAzqygHADOzinIAMDOrKAcAM7OKcgAwM6soBwAzs4pyADAzqygHADOzinIAMDOrKAcAM7OKcgAwM6uotgYASYdLCknLt7McZmZV1LYAIGk14MPAP9tVBjOzKmtnC+D7wFeBaGMZzMwqqy0BQNIuwDMRcV8T0x4saYqkKdOnT++D0pmZVcPgVmUs6QZg5TpJRwNHkbp/uhQRk4BJAOPHj3drwcysl7QsAETEhHqfS9oQWAO4TxLAqsA9kjaNiOdbVR4zM5tXywJAIxHxALBi7b2kqcD4iHihr8tiZlZlvg/AzKyi+rwFUBYRY9pdBjOzKnILwMysohwAzMwqygHAzKyiHADMzCrKAcDMrKIcAMzMKsoBwMysohwAzMwqygHAzKyiHADMzCrKAcDMrKIcAMzMKsoBwMysohwAzMwqygHAzKyiHADMzCrKAcDMrKLa/otgfWXqqTu1uwhmZv2KWwBmZhXlAGBmVlEOAGZmFeUAYGZWUQ4AZmYV5QBgZlZRDgBmZhXlAGBmVlEOAGZmFeUAYGZWUYqIdpehaZKmA0/1QlbLAy+0ML0vluH0aqf3hzI4vf11oFmjI2KF+T6NiMr9AVNamd4Xy3B6tdP7Qxmc3v460NM/dwGZmVWUA4CZWUVVNQBManF6XyzD6dVO7w9lcHp703tsQA0Cm5lZ76lqC8DMrPIcAMzMKqoyPwlZI2k/4GAggM9FxD2l9GuB9wKnR8RJpbSNgTOBOcDbwEER8UQhfRhwDTAbWBI4MiL+UKcM6wAPAR+MiFtLaW8Ad+S350fEuaX0TYBvAosCd0XEV0vp6wFn5beLA+tExHKFdAFnAJuQvv/vRcTkUh4nAB/K6/F54NvFbZLz+D9gHPAqsAQwtpD+LuAS4N3A9sCxpfk/BRwG/Ad4FhgBbFxI3xo4JW/juaQTlfUofSeS9if1k95Yyn8bYDLw9zzp4Tm/eb7XXI5P5/xHACMLeXyCVE8AVgKWBQYV0pcFLs7buLYfrVNIXxL4Bela7rmk+jA7r9NBeb3PBVYHZgLL5LRa+srAj4G1gbWAFSjVPWAisHt+/8+cVzF907yd5+byLVYsQ63u5u97f2Baaf6tgONI994sTdpn3ijOL+lrpLqyDDAEeKkw/67Af+dlr53/P15IBzgvl2+p/P6NQvprwM/zNn4OWAN4M2/LI0nfe60evpa/w9cL6VPpqIe7AcdT2DeBUXTUw3/n98X836ajHip/PovSvl2oh3eV8p9DRz0cBAwDXi7OX6iDi5K+82cL86/EvHXw4YjYnd7U6utM+9MfaSe+h7QjrAHcWmeaVYH9gGPqpK0MDM2vdyQdoIvpiwCD8+s1SQfoeuU4H7gB2KJO2mOdlH8x4PpaGZpY3z2As0ufbQDclF8PBR4vpY8Dfp9frwbcVN4mpIP6ufn1p4AfltKXJO2M5wFb1Jl/TWBQfv0t4Mul9MUK5TkA+FH5OyEdbK4kHVDK+W8D/KSz7xVYn3SAVr300rxnAZ8tzX8YcFxheVeU0r8IHJFfHwx8u1hvgEOBYwvb4Aul9OGkg+7NuWzz1T1g7UIZrwB2LqUXt+N3gS+V6y7pwDIZeLJO/sX1qbf8HYBTmtw3bgC2Lc3/HeDT+bMvAN8vpf8A2Ct/dgRwSHHfYt56+GngtFJ6sR5uSWnfZP56+JlSenH7HQh8p7xvM289LOe/DbkeUufYQKEO1kuvUwf36ukxsPxXtS6gTYE/RcTsiHgSGCpp8eIEEfF0o5kj4vmImJnfvkk6Myimz42I2mfDgPvLeUjaDHgeaLSclSX9UdJvJI0ppW1OOgO5UNKNkrZsVNbsk8AvS589C8yWtCgpALxUSl8HuDuvzzRSoJxemmZrUqUH+B3pzPodEfF6RLxUeP90Kf2JiJiT375Zzj8iZhfeDgNuq7NunwfOTpPX/c4+IulPks6QtESdaSaSzhavk3RZnfkByNtpB+BnpaRHctkgnViU71BfB5iSX18PfCC/rtWb4jacDLy/mB4Rr0bErFpm9epeRPyjsLxXSS2JYnpxO0I6+SmWAVLr7JvAnAZ1+1OSbgX+l3SWXUzfAxgi6Q+kVuIideZH0orAqIi4sZT+EKnlQJ736VJ6cRveQdpm0LFvFbfhFaQWyzvppXoY5X2zTj2cXUovbr+hwH2l5cO89bDevv8RSX8CTied5RfT36mDwKWkIFqev1gHf0svq1oAWI7UBKt5hXSG0C2SlgJOIlX6ctqovMNcB9Q7sBwNnNpJ9mMiYmtS8//cUtpIYCNgH2Bf4JzcHVOvjMsB6wJ/LiW9DPwDeBS4N69H0YPANpIWk7QR6exz2dI0xe34Sp30pkhal3QWd3GdtJ0kTSGded9eSlsW2CoirizPl91NOjveEphBamGUjczr8WHSdv5Og7x2AG6JiDfqLOP9kh4kdal9t5T+QF43SGe0I0r1prwNy+l11Zsmd5mtAtxSTpd0oKQHSGfADxXTJa0NLB0R9zfI/7fAe0gH2tHAPqX0kcDciNiOdIA+ssE67EXqiinnfwNwiKT7SS2in5TSy9twldK+Vd6GK3S27zXaNwv18E/l9FI9fKKYXq6HdfIv18MTSunlOvjDBuVvVAd7rGoB4CU6zjggNbPLZ8CdytH4YlJz8+FyekQ8ExFbkFobZ5bm3Yl0e/eLjfKPiBfy/2tJO125/LdFxIyIeIb0nJD5n++R7An8KnL7seBDpL7OtUgB4pRiKyiv04Wks9YvkM7Syi2A4nYczrxBtSmSViX17+4VEf8pp0fEVRExHjiG1A9bdCSpyV5XRMws5HkBML7OZC8B1+btcy2wYYPs6rWiAL4KXBoRGwAfJ3WDFZ1LOju+ibS9n2PeelNvGzasV1C/7kkaSzqh2Is0FjFPekScGxEbAr8GvlZKPx44sVH+EfFyRMzJZ8kXkep0eR2uybNfQzo5qbcO+wC/rFP+00hdTGNzWU4tpZ8CbCbpxrxuT5b2rfI2nN5o38vbYr59s1QPnyinl+rhF0rp89TDcv516uG765S/WAfXalD+RnWwx6oWAO4AtpC0qKTVgVkR8WazM0tahPRFXB4Rl9dJL3YnzaCjSV4zjnR2fQ3pQPwdSaML8y8taVB+PZb5HwR1B7COpMGShgIrAo2CyT7UrzQCXs479UzSuMKg4gQRcVZuhXwPeKDQTK75I+mMjPz/jw3KUJek5UlN3kMj4vE66UMKb18hNZOL1gGOyttxFUnztCAkDS+83ZaOweCim+kIDJuQ+nDL5RiW0+YbyCdtx9r3829KLcnczXhYRHyQ1D00jHnrTXEb7kQKyHXrVS7LfHVP0lrAT0kH/5fqpBe346uks9ziMtYknXVeQ2pBPFSav3iytC3wX6X5b6ZjG74PeFd5HfIFD0HavuV9p7gNp+ft8U567gbbNyK2JXXP/DpPW9u3itvwv+moh/X2vUULr2cAM4v1kHm7ZGvpxe33Gh31sJZ/o3pYm79YDz9MRz2szX8zHdvv/XTUwXfK30Ud7LHK3Qgm6QDSFQZBiuhTSunnkCr64sCDEbFrIW0iaUCpNs8DEfG5QvomwPdJo/+DgeOjzlVAedrzSANEtxY+25TU9TMzl+/zEXFfab59gUNIFfrUiKjX1F0TuCSfuZTTBpHOTtfK63h+RPxfaZrrcvlfJPX9nlzcJqQrKs4gXfkzI0+3SSH9U8BvSFfuPEM60RhSSH+adIXIY3mRi5CawrX0K0ldXHNJfbMv52XV+04eIw1UF8t3HWnw+HXSAeYAUpdCcZqPkQLcxnn5z5MG5d5ZRq4r60fE4eV6QeoSOJ8UPJcgHcDWKKQfRRq4m0O6smUbCvWG1IL4KamLTaSTg2L6GXn+TXJ+jwCfKE2zBqkV9zSpJbgmHVeQPUAKTNvl90NIFwA0qrvPkfq5i/nPACaQ+uPfJJ2ZFtO/DJxDulhgWdKVPncV85f0jbxtnqO075D6zn+c81+ZdBXTnYX0y0hjFHPz+o+lsG+RvvdaPYT0XbxZSL+Ljnr4MmlQ/YlC+sfoqIdL5/V/tpC+Bh31cPGc/+vU2bclTSMNpBfL92466uHbOY/ZhfQb6aiDw/K8s4r5F+sgLVC5AGBmZknVuoDMzCxzADAzqygHADOzinIAMDOrKAcAM7OKcgCwhZ6kXSVFvuOzFfmPk7Rjg7T9JM13U1IX+R3VOyUz65wDgFXB3sCt+X8rjKPjhqTe4ABgfcIBwBZqkpYmPZH0QNIds7XPF5F0lqS/Sbpe0tX5Rj8kbaL0QL67JV0raZX8+c2STpN0p6RHJW0paTHgG8Ceku6VtGedYqyW5/2HpOMKZfhkzuteST+WNEjSqcAS+bMLWrhpzBwAbKG3C3BNRGn/mosAAAHdSURBVDwKvJjv1oZ0N/MY0l2i+5KetFp7Hs4ZwMSI2IR0t+7JhfwGR8SmpMc9H5efGPl14OKIGBcR8z3YjnQH7e6kO1Y/Lmm8pPeQntf0gYgYR7oLdJ+IOAJ4I+e1Ty7T1ZJG9tYGMaup3A/CWOXsTXoUL6QHmu1NekrjFqSH5c0Fns8PbYN0+/4GwPVKD1odRHqMQc1v8v+7SQGkGdfXHgAo6Td52W+THvNwV17OEqRHN8wnInqze8nsHQ4AttCSNIL0ELMNJQXpYB6SvtLZbMBDEbF5g/TawwNrz3xpRvl5K5GX8/OIOLLJPMx6nbuAbGE2kfSwu9ERMSYiViM9sGtL0u8k7J7HAlYiPawN0hMbV5D0TpeQpPW7WM5M0oPEGvmQpBGSliA9fOzPpKc7TlT6sRRyeu3JsG/lriizlnIAsIXZ3sz/wyCX5s8vJT1F82HSY4rvAV7NffoTgdMk3Uf60Zz/6mI5NwHrdTIIfGde3v2k3xCYkp93fwzpF8nuJ/3+wip5+knA/bVBYI8BWKv4aaBWWZKWjohZSr+edidpQPb5dpfLrK94DMCq7Mr8oyeLASf64G9V4xaAmVlFeQzAzKyiHADMzCrKAcDMrKIcAMzMKsoBwMysov4fhiG9y87QbTMAAAAASUVORK5CYII=\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"# Deep Reinforcement Learning","metadata":{"id":"ebqIYd1-gaPO"}},{"cell_type":"markdown","source":"## MountainCar-v0","metadata":{"id":"EcqgshmAgixU"}},{"cell_type":"code","source":"class MountainCarTrain:\n    def __init__(self,env):\n        self.env=env\n        self.gamma=0.99\n\n        self.epsilon = 1\n        self.epsilon_decay = 0.05\n\n        self.epsilon_min=0.01\n\n\n        self.learingRate=0.001\n\n        self.replayBuffer=deque(maxlen=20000)\n        self.trainNetwork=self.createNetwork()\n\n        self.episodeNum=400\n\n        self.iterationNum=201 #max is 200\n\n        self.numPickFromBuffer=32\n\n        self.targetNetwork=self.createNetwork()\n\n        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n\n    def createNetwork(self):\n        model = models.Sequential()\n        state_shape = self.env.observation_space.shape\n\n        model.add(layers.Dense(24, activation='relu', input_shape=state_shape))\n        model.add(layers.Dense(48, activation='relu'))\n        model.add(layers.Dense(self.env.action_space.n,activation='linear'))\n        # model.compile(optimizer=optimizers.RMSprop(lr=self.learingRate), loss=losses.mean_squared_error)\n        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learingRate))\n        return model\n\n    def getBestAction(self,state):\n\n        self.epsilon = max(self.epsilon_min, self.epsilon)\n\n        if np.random.rand(1) < self.epsilon:\n            action = np.random.randint(0, 3)\n        else:\n            action=np.argmax(self.trainNetwork.predict(state)[0])\n\n        return action\n\n    \n\n    def trainFromBuffer_Boost(self):\n        if len(self.replayBuffer) < self.numPickFromBuffer:\n            return\n        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n        npsamples = np.array(samples)\n        states_temp, actions_temp, rewards_temp, newstates_temp, dones_temp = np.hsplit(npsamples, 5)\n        states = np.concatenate((np.squeeze(states_temp[:])), axis = 0)\n        rewards = rewards_temp.reshape(self.numPickFromBuffer,).astype(float)\n        targets = self.trainNetwork.predict(states)\n        newstates = np.concatenate(np.concatenate(newstates_temp))\n        dones = np.concatenate(dones_temp).astype(bool)\n        notdones = ~dones\n        notdones = notdones.astype(float)\n        dones = dones.astype(float)\n        Q_futures = self.targetNetwork.predict(newstates).max(axis = 1)\n        targets[(np.arange(self.numPickFromBuffer), actions_temp.reshape(self.numPickFromBuffer,).astype(int))] = rewards * dones + (rewards + Q_futures * self.gamma)*notdones\n        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n\n\n\n    def trainFromBuffer(self):\n        if len(self.replayBuffer) < self.numPickFromBuffer:\n            return\n\n        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n\n        states = []\n        newStates=[]\n        for sample in samples:\n            state, action, reward, new_state, done = sample\n            states.append(state)\n            newStates.append(new_state)\n\n        newArray = np.array(states)\n        states = newArray.reshape(self.numPickFromBuffer, 2)\n\n        newArray2 = np.array(newStates)\n        newStates = newArray2.reshape(self.numPickFromBuffer, 2)\n\n        targets = self.trainNetwork.predict(states)\n        new_state_targets=self.targetNetwork.predict(newStates)\n\n        i=0\n        for sample in samples:\n            state, action, reward, new_state, done = sample\n            target = targets[i]\n            if done:\n                target[action] = reward\n            else:\n                Q_future = max(new_state_targets[i])\n                target[action] = reward + Q_future * self.gamma\n            i+=1\n\n        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n\n\n    def orginalTry(self,currentState,eps):\n        rewardSum = 0\n        max_position=-99\n\n        for i in range(self.iterationNum):\n            bestAction = self.getBestAction(currentState)\n\n            new_state, reward, done, _ = env.step(bestAction)\n\n            new_state = new_state.reshape(1, 2)\n\n            # # Keep track of max position\n            if new_state[0][0] > max_position:\n                max_position = new_state[0][0]\n\n\n            # # Adjust reward for task completion\n            if new_state[0][0] >= 0.5:\n                reward += 10\n\n            self.replayBuffer.append([currentState, bestAction, reward, new_state, done])\n\n            #Or you can use self.trainFromBuffer_Boost(), it is a matrix wise version for boosting \n            self.trainFromBuffer()\n\n            rewardSum += reward\n\n            currentState = new_state\n\n            if done:\n                break\n\n        if i >= 199:\n            print(\"Failed to finish task in epsoide {}\".format(eps))\n        else:\n            print(\"Success in epsoide {}, used {} iterations!\".format(eps, i))\n            self.trainNetwork.save('./trainNetworkInEPS{}.h5'.format(eps))\n\n        #Sync\n        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n\n        print(\"now epsilon is {}, the reward is {} maxPosition is {}\".format(max(self.epsilon_min, self.epsilon), rewardSum,max_position))\n        self.epsilon -= self.epsilon_decay\n\n    def start(self):\n        for eps in range(self.episodeNum):\n            currentState=env.reset().reshape(1,2)\n            self.orginalTry(currentState, eps)","metadata":{"id":"edTKG3HcgeJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('MountainCar-v0')\ndqn=MountainCarTrain(env=env)\ndqn.start()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tEgKYtqgwz_","outputId":"d1e862d9-1408-4eec-a1d5-100b3bf30692"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Failed to finish task in epsoide 0\nnow epsilon is 1, the reward is -200.0 maxPosition is -0.3524132933850725\nFailed to finish task in epsoide 1\nnow epsilon is 0.95, the reward is -200.0 maxPosition is -0.3070783992907518\nFailed to finish task in epsoide 2\nnow epsilon is 0.8999999999999999, the reward is -200.0 maxPosition is -0.4299435894539687\nFailed to finish task in epsoide 3\nnow epsilon is 0.8499999999999999, the reward is -200.0 maxPosition is -0.43446207858135955\nFailed to finish task in epsoide 4\nnow epsilon is 0.7999999999999998, the reward is -200.0 maxPosition is -0.280338239602065\nFailed to finish task in epsoide 5\nnow epsilon is 0.7499999999999998, the reward is -200.0 maxPosition is -0.3681538690378387\nFailed to finish task in epsoide 6\nnow epsilon is 0.6999999999999997, the reward is -200.0 maxPosition is -0.36261997202587165\nFailed to finish task in epsoide 7\nnow epsilon is 0.6499999999999997, the reward is -200.0 maxPosition is -0.2033583982262082\nFailed to finish task in epsoide 8\nnow epsilon is 0.5999999999999996, the reward is -200.0 maxPosition is -0.13855569023469297\nFailed to finish task in epsoide 9\nnow epsilon is 0.5499999999999996, the reward is -200.0 maxPosition is -0.14842178324856836\nFailed to finish task in epsoide 10\nnow epsilon is 0.4999999999999996, the reward is -200.0 maxPosition is -0.2793319258043251\nFailed to finish task in epsoide 11\nnow epsilon is 0.4499999999999996, the reward is -200.0 maxPosition is -0.3795558553134636\nFailed to finish task in epsoide 12\nnow epsilon is 0.39999999999999963, the reward is -200.0 maxPosition is -0.15214968359420947\nFailed to finish task in epsoide 13\nnow epsilon is 0.34999999999999964, the reward is -200.0 maxPosition is -0.2348832047537745\nFailed to finish task in epsoide 14\nnow epsilon is 0.29999999999999966, the reward is -200.0 maxPosition is -0.1497673992059681\nFailed to finish task in epsoide 15\nnow epsilon is 0.24999999999999967, the reward is -200.0 maxPosition is -0.08413258119435982\nFailed to finish task in epsoide 16\nnow epsilon is 0.19999999999999968, the reward is -200.0 maxPosition is -0.2788637847690169\nFailed to finish task in epsoide 17\nnow epsilon is 0.1499999999999997, the reward is -200.0 maxPosition is -0.17541139327945746\nFailed to finish task in epsoide 18\nnow epsilon is 0.09999999999999969, the reward is -200.0 maxPosition is -0.23885933016467759\nFailed to finish task in epsoide 19\nnow epsilon is 0.049999999999999684, the reward is -200.0 maxPosition is -0.11590947866633974\nFailed to finish task in epsoide 20\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.12057596031550417\nSuccess in epsoide 21, used 185 iterations!\nnow epsilon is 0.01, the reward is -176.0 maxPosition is 0.5294146062416286\nFailed to finish task in epsoide 22\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.18958001342887487\nFailed to finish task in epsoide 23\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.07687881686420892\nFailed to finish task in epsoide 24\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.20245064840757712\nFailed to finish task in epsoide 25\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.07706479055932372\nFailed to finish task in epsoide 26\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.10456900634389282\nFailed to finish task in epsoide 27\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.02419682800432305\nFailed to finish task in epsoide 28\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3085194327013536\nFailed to finish task in epsoide 29\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1193437903467623\nFailed to finish task in epsoide 30\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.13010469556431345\nFailed to finish task in epsoide 31\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.253077924123337\nFailed to finish task in epsoide 32\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.04981205956334721\nFailed to finish task in epsoide 33\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.09580553163391546\nFailed to finish task in epsoide 34\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.004608990170606679\nFailed to finish task in epsoide 35\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2760154039240527\nFailed to finish task in epsoide 36\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.19128354963158592\nFailed to finish task in epsoide 37\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.03475348570020361\nFailed to finish task in epsoide 38\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.22268068276799002\nFailed to finish task in epsoide 39\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2033148717134867\nFailed to finish task in epsoide 40\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.2145653751979175\nFailed to finish task in epsoide 41\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.09969594676308417\nFailed to finish task in epsoide 42\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.04221954398141184\nFailed to finish task in epsoide 43\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.23829267350123073\nFailed to finish task in epsoide 44\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.22811383970325916\nFailed to finish task in epsoide 45\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.019928887042185605\nFailed to finish task in epsoide 46\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.16465049047824673\nFailed to finish task in epsoide 47\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.23211727866456927\nSuccess in epsoide 48, used 115 iterations!\nnow epsilon is 0.01, the reward is -106.0 maxPosition is 0.5146450313176049\nFailed to finish task in epsoide 49\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.03757323940397939\nFailed to finish task in epsoide 50\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1327045195014213\nFailed to finish task in epsoide 51\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.17859691787926024\nFailed to finish task in epsoide 52\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.28437722651457875\nFailed to finish task in epsoide 53\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.31784345259358654\nSuccess in epsoide 54, used 166 iterations!\nnow epsilon is 0.01, the reward is -157.0 maxPosition is 0.5187573452834087\nFailed to finish task in epsoide 55\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2495120525554502\nFailed to finish task in epsoide 56\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.026103184552185105\nFailed to finish task in epsoide 57\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1928315780895043\nFailed to finish task in epsoide 58\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.16206150286733176\nFailed to finish task in epsoide 59\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.12057070647127391\nSuccess in epsoide 60, used 98 iterations!\nnow epsilon is 0.01, the reward is -89.0 maxPosition is 0.5154675016263012\nFailed to finish task in epsoide 61\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3266907765543536\nSuccess in epsoide 62, used 115 iterations!\nnow epsilon is 0.01, the reward is -106.0 maxPosition is 0.5093562372516088\nSuccess in epsoide 63, used 102 iterations!\nnow epsilon is 0.01, the reward is -93.0 maxPosition is 0.5032665865268193\nFailed to finish task in epsoide 64\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.03158751851330652\nFailed to finish task in epsoide 65\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2424638410147817\nFailed to finish task in epsoide 66\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1019430074053843\nFailed to finish task in epsoide 67\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.14255415900949733\nFailed to finish task in epsoide 68\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.05053562716336016\nFailed to finish task in epsoide 69\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.14940601513888294\nSuccess in epsoide 70, used 158 iterations!\nnow epsilon is 0.01, the reward is -149.0 maxPosition is 0.5211466536320155\nFailed to finish task in epsoide 71\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.22828822537240048\nFailed to finish task in epsoide 72\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.28256147766800965\nFailed to finish task in epsoide 73\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3577043644844212\nFailed to finish task in epsoide 74\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.13450531822988185\nSuccess in epsoide 75, used 174 iterations!\nnow epsilon is 0.01, the reward is -165.0 maxPosition is 0.5059110852620284\nFailed to finish task in epsoide 76\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.138928734339838\nFailed to finish task in epsoide 77\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.27758137421887413\nFailed to finish task in epsoide 78\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.25317017857459756\nFailed to finish task in epsoide 79\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2258238267981899\nFailed to finish task in epsoide 80\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3130529841388198\nFailed to finish task in epsoide 81\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.08706571126531527\nFailed to finish task in epsoide 82\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.1462486634505181\nFailed to finish task in epsoide 83\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.34276160870772293\nFailed to finish task in epsoide 84\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.0870642490145532\nFailed to finish task in epsoide 85\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.01879483725826105\nFailed to finish task in epsoide 86\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3674493707721311\nFailed to finish task in epsoide 87\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.28638191156605713\nSuccess in epsoide 88, used 159 iterations!\nnow epsilon is 0.01, the reward is -150.0 maxPosition is 0.5076502699253835\nFailed to finish task in epsoide 89\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.35281639577528084\nFailed to finish task in epsoide 90\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2932727044705906\nFailed to finish task in epsoide 91\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.06956679438850356\nFailed to finish task in epsoide 92\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.26076986791356377\nFailed to finish task in epsoide 93\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2408647638434624\nFailed to finish task in epsoide 94\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.08280775649750768\nFailed to finish task in epsoide 95\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.16373455704236908\nFailed to finish task in epsoide 96\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1484855716729034\nFailed to finish task in epsoide 97\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.4066075962847191\nFailed to finish task in epsoide 98\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.03091052433703733\nFailed to finish task in epsoide 99\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2749867307157541\nFailed to finish task in epsoide 100\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.0013214572967789905\nFailed to finish task in epsoide 101\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2171181534465875\nFailed to finish task in epsoide 102\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1241058264474776\nFailed to finish task in epsoide 103\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.010188534873267888\nFailed to finish task in epsoide 104\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.0021787170758499654\nFailed to finish task in epsoide 105\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.4171489240764023\nFailed to finish task in epsoide 106\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.40799331971256275\nFailed to finish task in epsoide 107\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.10868328455358472\nFailed to finish task in epsoide 108\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.31992749019392\nSuccess in epsoide 109, used 152 iterations!\nnow epsilon is 0.01, the reward is -143.0 maxPosition is 0.5221052039242029\nFailed to finish task in epsoide 110\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.10003874232306688\nFailed to finish task in epsoide 111\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.02444470209764\nFailed to finish task in epsoide 112\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.15709576188779367\nFailed to finish task in epsoide 113\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.03843210057705962\nFailed to finish task in epsoide 114\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.04351552286369948\nFailed to finish task in epsoide 115\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.1468716207805081\nFailed to finish task in epsoide 116\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.07642418458642222\nFailed to finish task in epsoide 117\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.36747026615683426\nFailed to finish task in epsoide 118\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.15824449466394083\nFailed to finish task in epsoide 119\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.07298235077250485\nSuccess in epsoide 120, used 163 iterations!\nnow epsilon is 0.01, the reward is -154.0 maxPosition is 0.5250838395911595\nFailed to finish task in epsoide 121\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.00787213243487874\nFailed to finish task in epsoide 122\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.0854474665777644\nFailed to finish task in epsoide 123\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.1417763280201628\nSuccess in epsoide 124, used 132 iterations!\nnow epsilon is 0.01, the reward is -123.0 maxPosition is 0.5212711802880642\nFailed to finish task in epsoide 125\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.27775662796627576\nFailed to finish task in epsoide 126\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.19929083510645101\nSuccess in epsoide 127, used 195 iterations!\nnow epsilon is 0.01, the reward is -186.0 maxPosition is 0.5177758039357164\nFailed to finish task in epsoide 128\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.07281634191825924\nFailed to finish task in epsoide 129\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.23514054916909405\nFailed to finish task in epsoide 130\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.057916105681928925\nSuccess in epsoide 131, used 186 iterations!\nnow epsilon is 0.01, the reward is -177.0 maxPosition is 0.5191966956131646\nFailed to finish task in epsoide 132\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2431024558792686\nFailed to finish task in epsoide 133\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.33212900649025073\nFailed to finish task in epsoide 134\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.2896495091176193\nFailed to finish task in epsoide 135\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.23403085407605875\nFailed to finish task in epsoide 136\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.19508774569403695\nFailed to finish task in epsoide 137\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.10848715865766262\nFailed to finish task in epsoide 138\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3807164705646364\nFailed to finish task in epsoide 139\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.53637911462134\nFailed to finish task in epsoide 140\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.5067311088727282\nFailed to finish task in epsoide 141\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.04346658918499973\nFailed to finish task in epsoide 142\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.28551421884480604\nFailed to finish task in epsoide 143\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.16662953999750757\nFailed to finish task in epsoide 144\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1258727848972562\nSuccess in epsoide 145, used 186 iterations!\nnow epsilon is 0.01, the reward is -177.0 maxPosition is 0.5071286824350305\nFailed to finish task in epsoide 146\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.3527220887394995\nSuccess in epsoide 147, used 196 iterations!\nnow epsilon is 0.01, the reward is -187.0 maxPosition is 0.5025737952388928\nFailed to finish task in epsoide 148\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.34994944722049426\nFailed to finish task in epsoide 149\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1679413263924455\nFailed to finish task in epsoide 150\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.12593262817692463\nFailed to finish task in epsoide 151\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.5081660470660619\nFailed to finish task in epsoide 152\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.40813675021640883\nSuccess in epsoide 153, used 191 iterations!\nnow epsilon is 0.01, the reward is -182.0 maxPosition is 0.5001177961030273\nSuccess in epsoide 154, used 99 iterations!\nnow epsilon is 0.01, the reward is -90.0 maxPosition is 0.5158885869590966\nFailed to finish task in epsoide 155\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.08934228711530823\nFailed to finish task in epsoide 156\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.12987132939915952\nFailed to finish task in epsoide 157\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.3635141594455871\nFailed to finish task in epsoide 158\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.015729962296154067\nSuccess in epsoide 159, used 182 iterations!\nnow epsilon is 0.01, the reward is -173.0 maxPosition is 0.5245149251599214\nFailed to finish task in epsoide 160\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.13743351588489944\nFailed to finish task in epsoide 161\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.32877994226174195\nFailed to finish task in epsoide 162\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.39322571768888115\nFailed to finish task in epsoide 163\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.1328073667469478\nFailed to finish task in epsoide 164\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.007507882628279195\nFailed to finish task in epsoide 165\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.34710568506345274\nFailed to finish task in epsoide 166\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.050038344962540775\nFailed to finish task in epsoide 167\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.5323028992044582\nFailed to finish task in epsoide 168\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.1733235302403134\nFailed to finish task in epsoide 169\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.2723347978802802\nSuccess in epsoide 170, used 183 iterations!\nnow epsilon is 0.01, the reward is -174.0 maxPosition is 0.5085949577799603\nSuccess in epsoide 171, used 178 iterations!\nnow epsilon is 0.01, the reward is -169.0 maxPosition is 0.5156499066168116\nFailed to finish task in epsoide 172\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.29331676393455547\nFailed to finish task in epsoide 173\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.03734431939163221\nSuccess in epsoide 174, used 195 iterations!\nnow epsilon is 0.01, the reward is -186.0 maxPosition is 0.5173555183544168\nSuccess in epsoide 175, used 127 iterations!\nnow epsilon is 0.01, the reward is -118.0 maxPosition is 0.5076784760942776\nFailed to finish task in epsoide 176\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.46917394976471816\nSuccess in epsoide 177, used 179 iterations!\nnow epsilon is 0.01, the reward is -170.0 maxPosition is 0.5200948158319417\nFailed to finish task in epsoide 178\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.26599991121625777\nFailed to finish task in epsoide 179\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.40696723025268444\nSuccess in epsoide 180, used 176 iterations!\nnow epsilon is 0.01, the reward is -167.0 maxPosition is 0.5017570810916677\nSuccess in epsoide 181, used 184 iterations!\nnow epsilon is 0.01, the reward is -175.0 maxPosition is 0.5074174909892076\nSuccess in epsoide 182, used 185 iterations!\nnow epsilon is 0.01, the reward is -176.0 maxPosition is 0.5192736070568075\nSuccess in epsoide 183, used 195 iterations!\nnow epsilon is 0.01, the reward is -186.0 maxPosition is 0.5061002953511963\nSuccess in epsoide 184, used 168 iterations!\nnow epsilon is 0.01, the reward is -159.0 maxPosition is 0.5126335280808539\nSuccess in epsoide 185, used 135 iterations!\nnow epsilon is 0.01, the reward is -126.0 maxPosition is 0.5222766334213044\nFailed to finish task in epsoide 186\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.496170714535724\nSuccess in epsoide 187, used 188 iterations!\nnow epsilon is 0.01, the reward is -179.0 maxPosition is 0.5038491073612663\nSuccess in epsoide 188, used 189 iterations!\nnow epsilon is 0.01, the reward is -180.0 maxPosition is 0.5289076277013133\nSuccess in epsoide 189, used 173 iterations!\nnow epsilon is 0.01, the reward is -164.0 maxPosition is 0.5132240455375635\nSuccess in epsoide 190, used 132 iterations!\nnow epsilon is 0.01, the reward is -123.0 maxPosition is 0.5007067825792395\nSuccess in epsoide 191, used 137 iterations!\nnow epsilon is 0.01, the reward is -128.0 maxPosition is 0.5036285384244206\nSuccess in epsoide 192, used 146 iterations!\nnow epsilon is 0.01, the reward is -137.0 maxPosition is 0.5251084107491971\nFailed to finish task in epsoide 193\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.25280145222186906\nSuccess in epsoide 194, used 184 iterations!\nnow epsilon is 0.01, the reward is -175.0 maxPosition is 0.5168520693424925\nSuccess in epsoide 195, used 102 iterations!\nnow epsilon is 0.01, the reward is -93.0 maxPosition is 0.5147889152332306\nFailed to finish task in epsoide 196\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.3044876615782575\nSuccess in epsoide 197, used 176 iterations!\nnow epsilon is 0.01, the reward is -167.0 maxPosition is 0.5142995709631081\nFailed to finish task in epsoide 198\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.20051720120671024\nFailed to finish task in epsoide 199\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.30504320056441964\nSuccess in epsoide 200, used 116 iterations!\nnow epsilon is 0.01, the reward is -107.0 maxPosition is 0.5154271293080845\nFailed to finish task in epsoide 201\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.273408590436358\nSuccess in epsoide 202, used 128 iterations!\nnow epsilon is 0.01, the reward is -119.0 maxPosition is 0.5096890039957227\nFailed to finish task in epsoide 203\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.11326490299649283\nFailed to finish task in epsoide 204\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.2833137043151989\nSuccess in epsoide 205, used 121 iterations!\nnow epsilon is 0.01, the reward is -112.0 maxPosition is 0.5061840405109604\nFailed to finish task in epsoide 206\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.24257898087796734\nFailed to finish task in epsoide 207\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.05717395325635527\nSuccess in epsoide 208, used 166 iterations!\nnow epsilon is 0.01, the reward is -157.0 maxPosition is 0.508812141263922\nFailed to finish task in epsoide 209\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.12176754594095415\nFailed to finish task in epsoide 210\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.18307186721219373\nSuccess in epsoide 211, used 180 iterations!\nnow epsilon is 0.01, the reward is -171.0 maxPosition is 0.5088893895937298\nFailed to finish task in epsoide 212\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.13660276698026647\nFailed to finish task in epsoide 213\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.24082172003829\nFailed to finish task in epsoide 214\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.12325192443901363\nFailed to finish task in epsoide 215\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.29235619311248306\nFailed to finish task in epsoide 216\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.18075233896314713\nFailed to finish task in epsoide 217\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.11972948014929852\nFailed to finish task in epsoide 218\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.0016394815316916815\nSuccess in epsoide 219, used 179 iterations!\nnow epsilon is 0.01, the reward is -170.0 maxPosition is 0.5156499066168116\nFailed to finish task in epsoide 220\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.059449682514593136\nFailed to finish task in epsoide 221\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.03725454636448963\nFailed to finish task in epsoide 222\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.2522611171096353\nFailed to finish task in epsoide 223\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.04933299303819067\nFailed to finish task in epsoide 224\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.00456951959023612\nSuccess in epsoide 225, used 160 iterations!\nnow epsilon is 0.01, the reward is -151.0 maxPosition is 0.510738735443973\nSuccess in epsoide 226, used 103 iterations!\nnow epsilon is 0.01, the reward is -94.0 maxPosition is 0.5055861836853976\nSuccess in epsoide 227, used 181 iterations!\nnow epsilon is 0.01, the reward is -172.0 maxPosition is 0.5125805611439715\nSuccess in epsoide 228, used 183 iterations!\nnow epsilon is 0.01, the reward is -174.0 maxPosition is 0.5051443499346304\nSuccess in epsoide 229, used 155 iterations!\nnow epsilon is 0.01, the reward is -146.0 maxPosition is 0.5104080635903744\nSuccess in epsoide 230, used 151 iterations!\nnow epsilon is 0.01, the reward is -142.0 maxPosition is 0.5205603734740933\nFailed to finish task in epsoide 231\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.015384440032770482\nSuccess in epsoide 232, used 150 iterations!\nnow epsilon is 0.01, the reward is -141.0 maxPosition is 0.5225951874497831\nSuccess in epsoide 233, used 169 iterations!\nnow epsilon is 0.01, the reward is -160.0 maxPosition is 0.5326581113024246\nFailed to finish task in epsoide 234\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.22360406767354488\nSuccess in epsoide 235, used 170 iterations!\nnow epsilon is 0.01, the reward is -161.0 maxPosition is 0.523663181689715\nFailed to finish task in epsoide 236\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.4395651150509545\nSuccess in epsoide 237, used 152 iterations!\nnow epsilon is 0.01, the reward is -143.0 maxPosition is 0.5314932056475683\nFailed to finish task in epsoide 238\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.35087120088274204\nSuccess in epsoide 239, used 167 iterations!\nnow epsilon is 0.01, the reward is -158.0 maxPosition is 0.501399043641431\nFailed to finish task in epsoide 240\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.402662975951206\nSuccess in epsoide 241, used 170 iterations!\nnow epsilon is 0.01, the reward is -161.0 maxPosition is 0.512541364035775\nFailed to finish task in epsoide 242\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.08058398722110718\nFailed to finish task in epsoide 243\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.4572944086125372\nSuccess in epsoide 244, used 140 iterations!\nnow epsilon is 0.01, the reward is -131.0 maxPosition is 0.5052800100824854\nFailed to finish task in epsoide 245\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.38919377315275316\nFailed to finish task in epsoide 246\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.26831229200639123\nSuccess in epsoide 247, used 176 iterations!\nnow epsilon is 0.01, the reward is -167.0 maxPosition is 0.5059158386879964\nFailed to finish task in epsoide 248\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.424643607585222\nSuccess in epsoide 249, used 168 iterations!\nnow epsilon is 0.01, the reward is -159.0 maxPosition is 0.503106542999169\nSuccess in epsoide 250, used 171 iterations!\nnow epsilon is 0.01, the reward is -162.0 maxPosition is 0.5129109719935041\nFailed to finish task in epsoide 251\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.13167255120063995\nFailed to finish task in epsoide 252\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.18216387262067923\nSuccess in epsoide 253, used 163 iterations!\nnow epsilon is 0.01, the reward is -154.0 maxPosition is 0.5022357688707656\nFailed to finish task in epsoide 254\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.16014519112826636\nFailed to finish task in epsoide 255\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.16115114774040248\nSuccess in epsoide 256, used 161 iterations!\nnow epsilon is 0.01, the reward is -152.0 maxPosition is 0.5011844219837774\nSuccess in epsoide 257, used 193 iterations!\nnow epsilon is 0.01, the reward is -184.0 maxPosition is 0.5045368641193779\nFailed to finish task in epsoide 258\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.460610042494133\nFailed to finish task in epsoide 259\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.2434202588483171\nSuccess in epsoide 260, used 119 iterations!\nnow epsilon is 0.01, the reward is -110.0 maxPosition is 0.5022682003904714\nFailed to finish task in epsoide 261\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.4963203243129449\nSuccess in epsoide 262, used 84 iterations!\nnow epsilon is 0.01, the reward is -75.0 maxPosition is 0.5068621771946921\nSuccess in epsoide 263, used 162 iterations!\nnow epsilon is 0.01, the reward is -153.0 maxPosition is 0.510454900543527\nSuccess in epsoide 264, used 87 iterations!\nnow epsilon is 0.01, the reward is -78.0 maxPosition is 0.514679671207304\nSuccess in epsoide 265, used 153 iterations!\nnow epsilon is 0.01, the reward is -144.0 maxPosition is 0.5220490622322357\nSuccess in epsoide 266, used 154 iterations!\nnow epsilon is 0.01, the reward is -145.0 maxPosition is 0.5122627620704988\nSuccess in epsoide 267, used 155 iterations!\nnow epsilon is 0.01, the reward is -146.0 maxPosition is 0.5015085364567435\nSuccess in epsoide 268, used 154 iterations!\nnow epsilon is 0.01, the reward is -145.0 maxPosition is 0.5028316823875842\nSuccess in epsoide 269, used 151 iterations!\nnow epsilon is 0.01, the reward is -142.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 270, used 85 iterations!\nnow epsilon is 0.01, the reward is -76.0 maxPosition is 0.5018168325185327\nSuccess in epsoide 271, used 94 iterations!\nnow epsilon is 0.01, the reward is -85.0 maxPosition is 0.5011861669828531\nSuccess in epsoide 272, used 91 iterations!\nnow epsilon is 0.01, the reward is -82.0 maxPosition is 0.5004692093438424\nSuccess in epsoide 273, used 90 iterations!\nnow epsilon is 0.01, the reward is -81.0 maxPosition is 0.5190579681491923\nSuccess in epsoide 274, used 158 iterations!\nnow epsilon is 0.01, the reward is -149.0 maxPosition is 0.5396609947637298\nSuccess in epsoide 275, used 175 iterations!\nnow epsilon is 0.01, the reward is -166.0 maxPosition is 0.503780048166395\nFailed to finish task in epsoide 276\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.11993668135289244\nSuccess in epsoide 277, used 158 iterations!\nnow epsilon is 0.01, the reward is -149.0 maxPosition is 0.5368577983788596\nFailed to finish task in epsoide 278\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.4282744910229501\nSuccess in epsoide 279, used 168 iterations!\nnow epsilon is 0.01, the reward is -159.0 maxPosition is 0.5368577983788596\nFailed to finish task in epsoide 280\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.05490740260597486\nSuccess in epsoide 281, used 172 iterations!\nnow epsilon is 0.01, the reward is -163.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 282, used 183 iterations!\nnow epsilon is 0.01, the reward is -174.0 maxPosition is 0.5368577983788596\nFailed to finish task in epsoide 283\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.12029166277232908\nSuccess in epsoide 284, used 163 iterations!\nnow epsilon is 0.01, the reward is -154.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 285, used 123 iterations!\nnow epsilon is 0.01, the reward is -114.0 maxPosition is 0.5104924964055697\nSuccess in epsoide 286, used 90 iterations!\nnow epsilon is 0.01, the reward is -81.0 maxPosition is 0.5047868193997761\nSuccess in epsoide 287, used 84 iterations!\nnow epsilon is 0.01, the reward is -75.0 maxPosition is 0.5180044005560268\nSuccess in epsoide 288, used 155 iterations!\nnow epsilon is 0.01, the reward is -146.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 289, used 158 iterations!\nnow epsilon is 0.01, the reward is -149.0 maxPosition is 0.5066004166176113\nSuccess in epsoide 290, used 147 iterations!\nnow epsilon is 0.01, the reward is -138.0 maxPosition is 0.5043155639159533\nFailed to finish task in epsoide 291\nnow epsilon is 0.01, the reward is -200.0 maxPosition is 0.2869967940337662\nSuccess in epsoide 292, used 100 iterations!\nnow epsilon is 0.01, the reward is -91.0 maxPosition is 0.5037177311982022\nSuccess in epsoide 293, used 131 iterations!\nnow epsilon is 0.01, the reward is -122.0 maxPosition is 0.5153602627291055\nSuccess in epsoide 294, used 177 iterations!\nnow epsilon is 0.01, the reward is -168.0 maxPosition is 0.5178236928840169\nSuccess in epsoide 295, used 144 iterations!\nnow epsilon is 0.01, the reward is -135.0 maxPosition is 0.5162936237262619\nSuccess in epsoide 296, used 155 iterations!\nnow epsilon is 0.01, the reward is -146.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 297, used 92 iterations!\nnow epsilon is 0.01, the reward is -83.0 maxPosition is 0.5109950215918876\nSuccess in epsoide 298, used 150 iterations!\nnow epsilon is 0.01, the reward is -141.0 maxPosition is 0.502802491577734\nSuccess in epsoide 299, used 159 iterations!\nnow epsilon is 0.01, the reward is -150.0 maxPosition is 0.5126748855866896\nSuccess in epsoide 300, used 90 iterations!\nnow epsilon is 0.01, the reward is -81.0 maxPosition is 0.5010716742088425\nSuccess in epsoide 301, used 138 iterations!\nnow epsilon is 0.01, the reward is -129.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 302, used 164 iterations!\nnow epsilon is 0.01, the reward is -155.0 maxPosition is 0.5218396476447826\nSuccess in epsoide 303, used 170 iterations!\nnow epsilon is 0.01, the reward is -161.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 304, used 157 iterations!\nnow epsilon is 0.01, the reward is -148.0 maxPosition is 0.5307989070535666\nSuccess in epsoide 305, used 146 iterations!\nnow epsilon is 0.01, the reward is -137.0 maxPosition is 0.505662886654018\nSuccess in epsoide 306, used 137 iterations!\nnow epsilon is 0.01, the reward is -128.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 307, used 156 iterations!\nnow epsilon is 0.01, the reward is -147.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 308, used 141 iterations!\nnow epsilon is 0.01, the reward is -132.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 309, used 149 iterations!\nnow epsilon is 0.01, the reward is -140.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 310, used 144 iterations!\nnow epsilon is 0.01, the reward is -135.0 maxPosition is 0.5055352839757251\nSuccess in epsoide 311, used 154 iterations!\nnow epsilon is 0.01, the reward is -145.0 maxPosition is 0.5368577983788596\nFailed to finish task in epsoide 312\nnow epsilon is 0.01, the reward is -200.0 maxPosition is -0.1482236132131905\nSuccess in epsoide 313, used 154 iterations!\nnow epsilon is 0.01, the reward is -145.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 314, used 134 iterations!\nnow epsilon is 0.01, the reward is -125.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 315, used 149 iterations!\nnow epsilon is 0.01, the reward is -140.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 316, used 149 iterations!\nnow epsilon is 0.01, the reward is -140.0 maxPosition is 0.5168264933681779\nSuccess in epsoide 317, used 150 iterations!\nnow epsilon is 0.01, the reward is -141.0 maxPosition is 0.5164555154369531\nSuccess in epsoide 318, used 162 iterations!\nnow epsilon is 0.01, the reward is -153.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 319, used 167 iterations!\nnow epsilon is 0.01, the reward is -158.0 maxPosition is 0.5410568598079735\nSuccess in epsoide 320, used 138 iterations!\nnow epsilon is 0.01, the reward is -129.0 maxPosition is 0.5344569582148987\nSuccess in epsoide 321, used 128 iterations!\nnow epsilon is 0.01, the reward is -119.0 maxPosition is 0.5214169148439793\nSuccess in epsoide 322, used 89 iterations!\nnow epsilon is 0.01, the reward is -80.0 maxPosition is 0.5150715461162264\nSuccess in epsoide 323, used 87 iterations!\nnow epsilon is 0.01, the reward is -78.0 maxPosition is 0.5128036159395065\nSuccess in epsoide 324, used 133 iterations!\nnow epsilon is 0.01, the reward is -124.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 325, used 152 iterations!\nnow epsilon is 0.01, the reward is -143.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 326, used 111 iterations!\nnow epsilon is 0.01, the reward is -102.0 maxPosition is 0.5063570394381067\nSuccess in epsoide 327, used 112 iterations!\nnow epsilon is 0.01, the reward is -103.0 maxPosition is 0.5383601931733877\nSuccess in epsoide 328, used 121 iterations!\nnow epsilon is 0.01, the reward is -112.0 maxPosition is 0.5103238791603762\nSuccess in epsoide 329, used 111 iterations!\nnow epsilon is 0.01, the reward is -102.0 maxPosition is 0.505103877914227\nSuccess in epsoide 330, used 149 iterations!\nnow epsilon is 0.01, the reward is -140.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 331, used 135 iterations!\nnow epsilon is 0.01, the reward is -126.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 332, used 83 iterations!\nnow epsilon is 0.01, the reward is -74.0 maxPosition is 0.5115580699527941\nSuccess in epsoide 333, used 130 iterations!\nnow epsilon is 0.01, the reward is -121.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 334, used 138 iterations!\nnow epsilon is 0.01, the reward is -129.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 335, used 84 iterations!\nnow epsilon is 0.01, the reward is -75.0 maxPosition is 0.5169294727007353\nSuccess in epsoide 336, used 174 iterations!\nnow epsilon is 0.01, the reward is -165.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 337, used 110 iterations!\nnow epsilon is 0.01, the reward is -101.0 maxPosition is 0.5066575500348665\nSuccess in epsoide 338, used 112 iterations!\nnow epsilon is 0.01, the reward is -103.0 maxPosition is 0.5142626584307007\nSuccess in epsoide 339, used 84 iterations!\nnow epsilon is 0.01, the reward is -75.0 maxPosition is 0.5056961384935496\nSuccess in epsoide 340, used 146 iterations!\nnow epsilon is 0.01, the reward is -137.0 maxPosition is 0.5257126142082267\nSuccess in epsoide 341, used 153 iterations!\nnow epsilon is 0.01, the reward is -144.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 342, used 118 iterations!\nnow epsilon is 0.01, the reward is -109.0 maxPosition is 0.5022375123349831\nSuccess in epsoide 343, used 107 iterations!\nnow epsilon is 0.01, the reward is -98.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 344, used 126 iterations!\nnow epsilon is 0.01, the reward is -117.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 345, used 156 iterations!\nnow epsilon is 0.01, the reward is -147.0 maxPosition is 0.5215723838392536\nSuccess in epsoide 346, used 115 iterations!\nnow epsilon is 0.01, the reward is -106.0 maxPosition is 0.5350022315419667\nSuccess in epsoide 347, used 144 iterations!\nnow epsilon is 0.01, the reward is -135.0 maxPosition is 0.5129352477415754\nSuccess in epsoide 348, used 123 iterations!\nnow epsilon is 0.01, the reward is -114.0 maxPosition is 0.5255893747224784\nSuccess in epsoide 349, used 148 iterations!\nnow epsilon is 0.01, the reward is -139.0 maxPosition is 0.5129352477415754\nSuccess in epsoide 350, used 113 iterations!\nnow epsilon is 0.01, the reward is -104.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 351, used 117 iterations!\nnow epsilon is 0.01, the reward is -108.0 maxPosition is 0.5219072612042372\nSuccess in epsoide 352, used 142 iterations!\nnow epsilon is 0.01, the reward is -133.0 maxPosition is 0.531806603976885\nSuccess in epsoide 353, used 87 iterations!\nnow epsilon is 0.01, the reward is -78.0 maxPosition is 0.5074467006083948\nSuccess in epsoide 354, used 83 iterations!\nnow epsilon is 0.01, the reward is -74.0 maxPosition is 0.5094936491906414\nSuccess in epsoide 355, used 114 iterations!\nnow epsilon is 0.01, the reward is -105.0 maxPosition is 0.5220428176325399\nSuccess in epsoide 356, used 84 iterations!\nnow epsilon is 0.01, the reward is -75.0 maxPosition is 0.5069459992731968\nSuccess in epsoide 357, used 108 iterations!\nnow epsilon is 0.01, the reward is -99.0 maxPosition is 0.5136887664481644\nSuccess in epsoide 358, used 104 iterations!\nnow epsilon is 0.01, the reward is -95.0 maxPosition is 0.5068717433547105\nSuccess in epsoide 359, used 148 iterations!\nnow epsilon is 0.01, the reward is -139.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 360, used 139 iterations!\nnow epsilon is 0.01, the reward is -130.0 maxPosition is 0.5371390279228068\nSuccess in epsoide 361, used 169 iterations!\nnow epsilon is 0.01, the reward is -160.0 maxPosition is 0.5363251097334638\nSuccess in epsoide 362, used 141 iterations!\nnow epsilon is 0.01, the reward is -132.0 maxPosition is 0.5387670642806129\nSuccess in epsoide 363, used 152 iterations!\nnow epsilon is 0.01, the reward is -143.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 364, used 109 iterations!\nnow epsilon is 0.01, the reward is -100.0 maxPosition is 0.5103039789848584\nSuccess in epsoide 365, used 153 iterations!\nnow epsilon is 0.01, the reward is -144.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 366, used 110 iterations!\nnow epsilon is 0.01, the reward is -101.0 maxPosition is 0.5331773244315756\nSuccess in epsoide 367, used 121 iterations!\nnow epsilon is 0.01, the reward is -112.0 maxPosition is 0.5415510801349341\nSuccess in epsoide 368, used 116 iterations!\nnow epsilon is 0.01, the reward is -107.0 maxPosition is 0.5416077270794938\nSuccess in epsoide 369, used 107 iterations!\nnow epsilon is 0.01, the reward is -98.0 maxPosition is 0.5238782807602149\nSuccess in epsoide 370, used 125 iterations!\nnow epsilon is 0.01, the reward is -116.0 maxPosition is 0.524843358283106\nSuccess in epsoide 371, used 113 iterations!\nnow epsilon is 0.01, the reward is -104.0 maxPosition is 0.5082745260400259\nSuccess in epsoide 372, used 109 iterations!\nnow epsilon is 0.01, the reward is -100.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 373, used 111 iterations!\nnow epsilon is 0.01, the reward is -102.0 maxPosition is 0.5334419889449963\nSuccess in epsoide 374, used 118 iterations!\nnow epsilon is 0.01, the reward is -109.0 maxPosition is 0.5126415094929134\nSuccess in epsoide 375, used 115 iterations!\nnow epsilon is 0.01, the reward is -106.0 maxPosition is 0.5288914748242523\nSuccess in epsoide 376, used 170 iterations!\nnow epsilon is 0.01, the reward is -161.0 maxPosition is 0.5420727081567481\nSuccess in epsoide 377, used 139 iterations!\nnow epsilon is 0.01, the reward is -130.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 378, used 112 iterations!\nnow epsilon is 0.01, the reward is -103.0 maxPosition is 0.5044215392055983\nSuccess in epsoide 379, used 120 iterations!\nnow epsilon is 0.01, the reward is -111.0 maxPosition is 0.534332189475921\nSuccess in epsoide 380, used 153 iterations!\nnow epsilon is 0.01, the reward is -144.0 maxPosition is 0.5171996084428573\nSuccess in epsoide 381, used 119 iterations!\nnow epsilon is 0.01, the reward is -110.0 maxPosition is 0.5201790005648352\nSuccess in epsoide 382, used 116 iterations!\nnow epsilon is 0.01, the reward is -107.0 maxPosition is 0.5327995356778372\nSuccess in epsoide 383, used 118 iterations!\nnow epsilon is 0.01, the reward is -109.0 maxPosition is 0.5202960534529724\nSuccess in epsoide 384, used 114 iterations!\nnow epsilon is 0.01, the reward is -105.0 maxPosition is 0.5263137791606775\nSuccess in epsoide 385, used 193 iterations!\nnow epsilon is 0.01, the reward is -184.0 maxPosition is 0.538779943878305\nSuccess in epsoide 386, used 135 iterations!\nnow epsilon is 0.01, the reward is -126.0 maxPosition is 0.5016541452092518\nSuccess in epsoide 387, used 119 iterations!\nnow epsilon is 0.01, the reward is -110.0 maxPosition is 0.5121105382877043\nSuccess in epsoide 388, used 110 iterations!\nnow epsilon is 0.01, the reward is -101.0 maxPosition is 0.5028131788767619\nSuccess in epsoide 389, used 113 iterations!\nnow epsilon is 0.01, the reward is -104.0 maxPosition is 0.5156746093942552\nSuccess in epsoide 390, used 112 iterations!\nnow epsilon is 0.01, the reward is -103.0 maxPosition is 0.5220196017047704\nSuccess in epsoide 391, used 179 iterations!\nnow epsilon is 0.01, the reward is -170.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 392, used 176 iterations!\nnow epsilon is 0.01, the reward is -167.0 maxPosition is 0.5368577983788596\nSuccess in epsoide 393, used 112 iterations!\nnow epsilon is 0.01, the reward is -103.0 maxPosition is 0.5089490934484102\nSuccess in epsoide 394, used 111 iterations!\nnow epsilon is 0.01, the reward is -102.0 maxPosition is 0.5203538584174375\nSuccess in epsoide 395, used 115 iterations!\nnow epsilon is 0.01, the reward is -106.0 maxPosition is 0.5253030230069918\nSuccess in epsoide 396, used 109 iterations!\nnow epsilon is 0.01, the reward is -100.0 maxPosition is 0.502343098735524\nSuccess in epsoide 397, used 123 iterations!\nnow epsilon is 0.01, the reward is -114.0 maxPosition is 0.506910749017576\nSuccess in epsoide 398, used 113 iterations!\nnow epsilon is 0.01, the reward is -104.0 maxPosition is 0.5239921412095027\nSuccess in epsoide 399, used 114 iterations!\nnow epsilon is 0.01, the reward is -105.0 maxPosition is 0.5186724733350173\n"}]},{"cell_type":"code","source":"env = gym.make('MountainCar-v0')\n\n#play 20 times\n#load the network\nmodel=models.load_model('trainNetworkInEPS399.h5')\n\ncompleted = 0\nnum_episodes = 20\n\nfor i_episode in range(num_episodes):\n\n    currentState = env.reset().reshape(1, 2)\n\n    print(\"============================================\")\n\n    rewardSum=0\n    done = False\n    t = 0\n    while not done:\n        # env.render()\n        action = np.argmax(model.predict(currentState)[0])\n\n        new_state, reward, done, info = env.step(action)\n\n        new_state = new_state.reshape(1, 2)\n\n        currentState=new_state\n\n        rewardSum+=reward\n        \n        t+=1\n\n        if t == 200 :\n          print(\"Episode finished but couldnot reach the top of the hill\")\n          break\n\n        if done:\n            completed +=1 \n            print(\"Episode finished after {} timesteps reward is {}\".format(t,rewardSum))\n            break\n\nprint(f\"Among {num_episodes} , {completed} episodes were completed and able to reach the top of the hill\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmFLfYqcg22U","outputId":"53af131a-f8ee-4f9f-d846-9f09acc6fb05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"============================================\nEpisode finished after 110 timesteps reward is -110.0\n============================================\nEpisode finished but couldnot reach the top of the hill\n============================================\nEpisode finished after 122 timesteps reward is -122.0\n============================================\nEpisode finished but couldnot reach the top of the hill\n============================================\nEpisode finished after 116 timesteps reward is -116.0\n============================================\nEpisode finished after 118 timesteps reward is -118.0\n============================================\nEpisode finished after 116 timesteps reward is -116.0\n============================================\nEpisode finished after 110 timesteps reward is -110.0\n============================================\nEpisode finished after 109 timesteps reward is -109.0\n============================================\nEpisode finished after 116 timesteps reward is -116.0\n============================================\nEpisode finished after 111 timesteps reward is -111.0\n============================================\nEpisode finished after 112 timesteps reward is -112.0\n============================================\nEpisode finished after 110 timesteps reward is -110.0\n============================================\nEpisode finished after 115 timesteps reward is -115.0\n============================================\nEpisode finished after 121 timesteps reward is -121.0\n============================================\nEpisode finished after 116 timesteps reward is -116.0\n============================================\nEpisode finished after 115 timesteps reward is -115.0\n============================================\nEpisode finished after 115 timesteps reward is -115.0\n============================================\nEpisode finished after 112 timesteps reward is -112.0\n============================================\nEpisode finished after 112 timesteps reward is -112.0\nAmong 20 , 18 episodes were completed and able to reach the top of the hill\n"}]},{"cell_type":"markdown","source":"## Roulette","metadata":{"id":"TA8op3jTaamg"}},{"cell_type":"code","source":"class RouletteTrain:\n    def __init__(self,env):\n        self.env=env\n        self.gamma=0.99\n\n        self.epsilon = 1\n        self.epsilon_decay = 0.05\n\n        self.epsilon_min=0.01\n\n\n        self.learingRate=0.001\n\n        self.replayBuffer=deque(maxlen=20000)\n        self.trainNetwork=self.createNetwork()\n\n        self.episodeNum=100\n\n        self.iterationNum=201 #max is 200\n\n        self.numPickFromBuffer=32\n\n        self.targetNetwork=self.createNetwork()\n\n        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n\n    def createNetwork(self):\n        model = models.Sequential()\n        state_shape = self.env.observation_space.shape\n\n        model.add(layers.Dense(24, activation='relu', input_shape=(1,1)))\n        model.add(layers.Dense(48, activation='relu'))\n        model.add(layers.Dense(self.env.action_space.n,activation='linear'))\n        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learingRate))\n        return model\n\n    def getBestAction(self,state):\n        self.epsilon = max(self.epsilon_min, self.epsilon)\n\n        if np.random.rand(1) < self.epsilon:\n            action = np.random.randint(0, 3)\n        else:\n            action=np.argmax(self.trainNetwork.predict(state)[0])\n\n        return action\n\n    \n\n    def trainFromBuffer_Boost(self):\n        if len(self.replayBuffer) < self.numPickFromBuffer:\n            return\n        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n        npsamples = np.array(samples)\n        states_temp, actions_temp, rewards_temp, newstates_temp, dones_temp = np.hsplit(npsamples, 5)\n        states = np.concatenate((np.squeeze(states_temp[:])), axis = 0)\n        rewards = rewards_temp.reshape(self.numPickFromBuffer,).astype(float)\n        targets = self.trainNetwork.predict(states)\n        newstates = np.concatenate(np.concatenate(newstates_temp))\n        dones = np.concatenate(dones_temp).astype(bool)\n        notdones = ~dones\n        notdones = notdones.astype(float)\n        dones = dones.astype(float)\n        Q_futures = self.targetNetwork.predict(newstates).max(axis = 1)\n        targets[(np.arange(self.numPickFromBuffer), actions_temp.reshape(self.numPickFromBuffer,).astype(int))] = rewards * dones + (rewards + Q_futures * self.gamma)*notdones\n        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n\n\n\n    def trainFromBuffer(self):\n        if len(self.replayBuffer) < self.numPickFromBuffer:\n            return\n\n        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n        states = []\n        newStates=[]\n        for sample in samples:\n            state, action, reward, new_state, done = sample\n            states.append(state)\n            newStates.append(new_state)\n\n        newArray = np.array(states)\n        states = newArray.reshape(self.numPickFromBuffer, 1)\n\n        newArray2 = np.array(newStates)\n        newStates = newArray2.reshape(self.numPickFromBuffer, 1)\n\n        targets = self.trainNetwork.predict(states)\n        new_state_targets=self.targetNetwork.predict(newStates)\n\n        i=0\n        for sample in samples:\n            state, action, reward, new_state, done = sample\n            target = targets[i]\n            if done:\n                target[0][action] = reward\n            else:\n                Q_future = max(new_state_targets[i][0])\n                target[0][action] = reward + Q_future * self.gamma\n            i+=1\n\n        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n\n\n    def orginalTry(self,currentState,eps):\n        rewardSum = 0\n\n        for i in range(self.iterationNum):\n            bestAction = self.getBestAction(currentState)\n\n            nw_state, reward, done, _ = env.step(bestAction)\n\n            new_state = np.zeros((1,1),dtype=np.float64)\n            new_state[0] = nw_state\n\n            self.replayBuffer.append([currentState, bestAction, reward, new_state, done])\n\n            #Or you can use self.trainFromBuffer_Boost(), it is a matrix wise version for boosting \n            self.trainFromBuffer()\n\n            rewardSum += reward\n\n            currentState = new_state\n\n            if done:\n                break\n\n        print(\"Success in epsoide {}, used {} iterations!\".format(eps, i))\n        self.trainNetwork.save('./trainNetworkInEPS{}.h5'.format(eps))\n\n        #Sync\n        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n\n        print(\"now epsilon is {}, the reward is {}\".format(max(self.epsilon_min, self.epsilon), rewardSum))\n        self.epsilon -= self.epsilon_decay\n\n    def start(self):\n        for eps in range(self.episodeNum):\n            a = env.reset()\n            currentState=np.zeros((1,1),dtype=np.float64)\n            currentState[0] = a\n            self.orginalTry(currentState, eps)","metadata":{"id":"bPvrr_HkahR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('Roulette-v0')\ndqn=RouletteTrain(env=env)\ndqn.start()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOJewGPralDa","outputId":"726fa93c-5dd6-4e8d-e283-c144c5d02461"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Success in epsoide 0, used 99 iterations!\nnow epsilon is 1, the reward is 13.0\nSuccess in epsoide 1, used 99 iterations!\nnow epsilon is 0.95, the reward is 1.0\nSuccess in epsoide 2, used 99 iterations!\nnow epsilon is 0.8999999999999999, the reward is -32.0\nSuccess in epsoide 3, used 99 iterations!\nnow epsilon is 0.8499999999999999, the reward is -28.0\nSuccess in epsoide 4, used 99 iterations!\nnow epsilon is 0.7999999999999998, the reward is -8.0\nSuccess in epsoide 5, used 99 iterations!\nnow epsilon is 0.7499999999999998, the reward is -32.0\nSuccess in epsoide 6, used 99 iterations!\nnow epsilon is 0.6999999999999997, the reward is 13.0\nSuccess in epsoide 7, used 99 iterations!\nnow epsilon is 0.6499999999999997, the reward is -6.0\nSuccess in epsoide 8, used 99 iterations!\nnow epsilon is 0.5999999999999996, the reward is -8.0\nSuccess in epsoide 9, used 99 iterations!\nnow epsilon is 0.5499999999999996, the reward is -8.0\nSuccess in epsoide 10, used 99 iterations!\nnow epsilon is 0.4999999999999996, the reward is 44.0\nSuccess in epsoide 11, used 99 iterations!\nnow epsilon is 0.4499999999999996, the reward is 15.0\nSuccess in epsoide 12, used 99 iterations!\nnow epsilon is 0.39999999999999963, the reward is -18.0\nSuccess in epsoide 13, used 99 iterations!\nnow epsilon is 0.34999999999999964, the reward is 21.0\nSuccess in epsoide 14, used 99 iterations!\nnow epsilon is 0.29999999999999966, the reward is -12.0\nSuccess in epsoide 15, used 99 iterations!\nnow epsilon is 0.24999999999999967, the reward is -12.0\nSuccess in epsoide 16, used 99 iterations!\nnow epsilon is 0.19999999999999968, the reward is 45.0\nSuccess in epsoide 17, used 99 iterations!\nnow epsilon is 0.1499999999999997, the reward is -12.0\nSuccess in epsoide 18, used 99 iterations!\nnow epsilon is 0.09999999999999969, the reward is 12.0\nSuccess in epsoide 19, used 99 iterations!\nnow epsilon is 0.049999999999999684, the reward is 6.0\nSuccess in epsoide 20, used 99 iterations!\nnow epsilon is 0.01, the reward is -8.0\nSuccess in epsoide 21, used 99 iterations!\nnow epsilon is 0.01, the reward is 10.0\nSuccess in epsoide 22, used 99 iterations!\nnow epsilon is 0.01, the reward is -6.0\nSuccess in epsoide 23, used 99 iterations!\nnow epsilon is 0.01, the reward is -22.0\nSuccess in epsoide 24, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 25, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 26, used 99 iterations!\nnow epsilon is 0.01, the reward is -12.0\nSuccess in epsoide 27, used 99 iterations!\nnow epsilon is 0.01, the reward is 10.0\nSuccess in epsoide 28, used 99 iterations!\nnow epsilon is 0.01, the reward is -8.0\nSuccess in epsoide 29, used 99 iterations!\nnow epsilon is 0.01, the reward is -6.0\nSuccess in epsoide 30, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\nSuccess in epsoide 31, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 32, used 99 iterations!\nnow epsilon is 0.01, the reward is -14.0\nSuccess in epsoide 33, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 34, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 35, used 99 iterations!\nnow epsilon is 0.01, the reward is -16.0\nSuccess in epsoide 36, used 99 iterations!\nnow epsilon is 0.01, the reward is 2.0\nSuccess in epsoide 37, used 99 iterations!\nnow epsilon is 0.01, the reward is -8.0\nSuccess in epsoide 38, used 99 iterations!\nnow epsilon is 0.01, the reward is -12.0\nSuccess in epsoide 39, used 99 iterations!\nnow epsilon is 0.01, the reward is -2.0\nSuccess in epsoide 40, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 41, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\nSuccess in epsoide 42, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 43, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\nSuccess in epsoide 44, used 99 iterations!\nnow epsilon is 0.01, the reward is 4.0\nSuccess in epsoide 45, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 46, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 47, used 99 iterations!\nnow epsilon is 0.01, the reward is 18.0\nSuccess in epsoide 48, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 49, used 99 iterations!\nnow epsilon is 0.01, the reward is -8.0\nSuccess in epsoide 50, used 99 iterations!\nnow epsilon is 0.01, the reward is 10.0\nSuccess in epsoide 51, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 52, used 99 iterations!\nnow epsilon is 0.01, the reward is 14.0\nSuccess in epsoide 53, used 99 iterations!\nnow epsilon is 0.01, the reward is 2.0\nSuccess in epsoide 54, used 99 iterations!\nnow epsilon is 0.01, the reward is -2.0\nSuccess in epsoide 55, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 56, used 99 iterations!\nnow epsilon is 0.01, the reward is -14.0\nSuccess in epsoide 57, used 99 iterations!\nnow epsilon is 0.01, the reward is -8.0\nSuccess in epsoide 58, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\nSuccess in epsoide 59, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\nSuccess in epsoide 60, used 99 iterations!\nnow epsilon is 0.01, the reward is 2.0\nSuccess in epsoide 61, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 62, used 99 iterations!\nnow epsilon is 0.01, the reward is -2.0\nSuccess in epsoide 63, used 99 iterations!\nnow epsilon is 0.01, the reward is 6.0\nSuccess in epsoide 64, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 65, used 99 iterations!\nnow epsilon is 0.01, the reward is -18.0\nSuccess in epsoide 66, used 99 iterations!\nnow epsilon is 0.01, the reward is -14.0\nSuccess in epsoide 67, used 99 iterations!\nnow epsilon is 0.01, the reward is -20.0\nSuccess in epsoide 68, used 99 iterations!\nnow epsilon is 0.01, the reward is 6.0\nSuccess in epsoide 69, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 70, used 99 iterations!\nnow epsilon is 0.01, the reward is -6.0\nSuccess in epsoide 71, used 99 iterations!\nnow epsilon is 0.01, the reward is -16.0\nSuccess in epsoide 72, used 99 iterations!\nnow epsilon is 0.01, the reward is -6.0\nSuccess in epsoide 73, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 74, used 99 iterations!\nnow epsilon is 0.01, the reward is 16.0\nSuccess in epsoide 75, used 99 iterations!\nnow epsilon is 0.01, the reward is -6.0\nSuccess in epsoide 76, used 99 iterations!\nnow epsilon is 0.01, the reward is -18.0\nSuccess in epsoide 77, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 78, used 99 iterations!\nnow epsilon is 0.01, the reward is 12.0\nSuccess in epsoide 79, used 99 iterations!\nnow epsilon is 0.01, the reward is 14.0\nSuccess in epsoide 80, used 99 iterations!\nnow epsilon is 0.01, the reward is -18.0\nSuccess in epsoide 81, used 99 iterations!\nnow epsilon is 0.01, the reward is 2.0\nSuccess in epsoide 82, used 99 iterations!\nnow epsilon is 0.01, the reward is -12.0\nSuccess in epsoide 83, used 99 iterations!\nnow epsilon is 0.01, the reward is 4.0\nSuccess in epsoide 84, used 99 iterations!\nnow epsilon is 0.01, the reward is -12.0\nSuccess in epsoide 85, used 99 iterations!\nnow epsilon is 0.01, the reward is 10.0\nSuccess in epsoide 86, used 99 iterations!\nnow epsilon is 0.01, the reward is -6.0\nSuccess in epsoide 87, used 99 iterations!\nnow epsilon is 0.01, the reward is 20.0\nSuccess in epsoide 88, used 99 iterations!\nnow epsilon is 0.01, the reward is -10.0\nSuccess in epsoide 89, used 99 iterations!\nnow epsilon is 0.01, the reward is -12.0\nSuccess in epsoide 90, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\nSuccess in epsoide 91, used 99 iterations!\nnow epsilon is 0.01, the reward is -2.0\nSuccess in epsoide 92, used 99 iterations!\nnow epsilon is 0.01, the reward is 12.0\nSuccess in epsoide 93, used 99 iterations!\nnow epsilon is 0.01, the reward is -14.0\nSuccess in epsoide 94, used 99 iterations!\nnow epsilon is 0.01, the reward is 0.0\nSuccess in epsoide 95, used 99 iterations!\nnow epsilon is 0.01, the reward is 14.0\nSuccess in epsoide 96, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 97, used 99 iterations!\nnow epsilon is 0.01, the reward is 2.0\nSuccess in epsoide 98, used 99 iterations!\nnow epsilon is 0.01, the reward is 8.0\nSuccess in epsoide 99, used 99 iterations!\nnow epsilon is 0.01, the reward is -4.0\n"}]},{"cell_type":"code","source":"# After training it we save those models in which we now take the latest model to use.\n\nenv = gym.make('Roulette-v0')\n\n#play 20 times\n#load the network\nmodel=models.load_model('trainNetworkInEPS99.h5')\n\nnum_episodes = 20\ntotalReward = 0\n\nfor i_episode in range(num_episodes):\n\n    a = env.reset()\n    currentState=np.zeros((1,1),dtype=np.float64)\n    currentState[0] = a\n\n    print(\"============================================\")\n\n    rewardSum=0\n    done = False\n    t = 0\n    while not done:\n        # env.render()\n        action = np.argmax(model.predict(currentState)[0])\n\n        nw_state, reward, done, info = env.step(action)\n\n        new_state = np.zeros((1,1),dtype=np.float64)\n        new_state[0] = nw_state\n\n        currentState=new_state\n\n        rewardSum+=reward\n        \n        t+=1\n\n        if done:\n            totalReward += rewardSum\n            print(\"Episode finished after {} timesteps reward is {}\".format(t,rewardSum))\n            break\n\n\navg_rewards = int(totalReward/num_episodes)\nprint(\"Average reward points in {} episodes is {}\".format(num_episodes,avg_rewards))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ntxo1aZAasHu","outputId":"c8dbaded-58c3-45a4-e61b-cc52fb6afe1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"============================================\nEpisode finished after 100 timesteps reward is -22.0\n============================================\nEpisode finished after 100 timesteps reward is -4.0\n============================================\nEpisode finished after 100 timesteps reward is -8.0\n============================================\nEpisode finished after 100 timesteps reward is -4.0\n============================================\nEpisode finished after 100 timesteps reward is -2.0\n============================================\nEpisode finished after 100 timesteps reward is 0.0\n============================================\nEpisode finished after 100 timesteps reward is -14.0\n============================================\nEpisode finished after 100 timesteps reward is 0.0\n============================================\nEpisode finished after 100 timesteps reward is -16.0\n============================================\nEpisode finished after 100 timesteps reward is -8.0\n============================================\nEpisode finished after 100 timesteps reward is 10.0\n============================================\nEpisode finished after 100 timesteps reward is -10.0\n============================================\nEpisode finished after 100 timesteps reward is 4.0\n============================================\nEpisode finished after 100 timesteps reward is -2.0\n============================================\nEpisode finished after 100 timesteps reward is 4.0\n============================================\nEpisode finished after 100 timesteps reward is -6.0\n============================================\nEpisode finished after 100 timesteps reward is 2.0\n============================================\nEpisode finished after 100 timesteps reward is -12.0\n============================================\nEpisode finished after 100 timesteps reward is 4.0\n============================================\nEpisode finished after 100 timesteps reward is 4.0\nAverage reward points in 20 episodes is -4\n"}]}]}